{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "forwarded symbol: A\n",
      "forwarded symbol: T\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20-20-20\n",
    "Try the model that has shape (20, 20, 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # used in qfunc and runner.\n",
    "env = guided_environments.GuidedMountainCar(position_reward_factor=2.0, speed_reward_factor=5.0)\n",
    "qfunc = qfunc_impl.DQN(\n",
    "    model=qfunc_impl.CreateModel(\n",
    "        state_shape=env.GetStateShape(),\n",
    "        action_space_size=env.GetActionSpaceSize(),\n",
    "        hidden_layer_sizes=(20, 20, 20)),\n",
    "    training_batch_size=batch_size,\n",
    "    discount_factor=0.99,\n",
    ")\n",
    "runner = runner_impl.ExperienceReplayRunner(experience_capacity=100000, experience_sample_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:43:17] q_base.py:429 Episode 100/2000: avg_reward = -400.93, avg_steps=199.00 (over 100 episodes)\n",
      "[03:44:02] q_base.py:429 Episode 200/2000: avg_reward = -393.43, avg_steps=199.00 (over 100 episodes)\n",
      "[03:44:40] q_base.py:429 Episode 300/2000: avg_reward = -385.94, avg_steps=199.00 (over 100 episodes)\n",
      "[03:45:18] q_base.py:429 Episode 400/2000: avg_reward = -390.23, avg_steps=199.00 (over 100 episodes)\n",
      "[03:45:56] q_base.py:429 Episode 500/2000: avg_reward = -402.02, avg_steps=199.00 (over 100 episodes)\n",
      "[03:46:31] q_base.py:429 Episode 600/2000: avg_reward = -374.92, avg_steps=199.00 (over 100 episodes)\n",
      "[03:47:07] q_base.py:429 Episode 700/2000: avg_reward = -370.26, avg_steps=199.00 (over 100 episodes)\n",
      "[03:47:46] q_base.py:429 Episode 800/2000: avg_reward = -367.54, avg_steps=199.00 (over 100 episodes)\n",
      "[03:48:23] q_base.py:429 Episode 900/2000: avg_reward = -364.48, avg_steps=199.00 (over 100 episodes)\n",
      "[03:48:59] q_base.py:424 Episode 1000/2000: avg_reward = -381.26, avg_steps=199.00 (over 1000 episodes)\n",
      "[03:49:34] q_base.py:429 Episode 1100/2000: avg_reward = -360.08, avg_steps=199.00 (over 100 episodes)\n",
      "[03:50:13] q_base.py:429 Episode 1200/2000: avg_reward = -358.87, avg_steps=199.00 (over 100 episodes)\n",
      "[03:50:50] q_base.py:429 Episode 1300/2000: avg_reward = -357.21, avg_steps=199.00 (over 100 episodes)\n",
      "[03:51:27] q_base.py:429 Episode 1400/2000: avg_reward = -355.25, avg_steps=199.00 (over 100 episodes)\n",
      "[03:52:06] q_base.py:429 Episode 1500/2000: avg_reward = -354.57, avg_steps=199.00 (over 100 episodes)\n",
      "[03:52:42] q_base.py:429 Episode 1600/2000: avg_reward = -352.61, avg_steps=199.00 (over 100 episodes)\n",
      "[03:53:19] q_base.py:429 Episode 1700/2000: avg_reward = -352.85, avg_steps=199.00 (over 100 episodes)\n",
      "[03:53:57] q_base.py:429 Episode 1800/2000: avg_reward = -351.96, avg_steps=199.00 (over 100 episodes)\n",
      "[03:54:34] q_base.py:429 Episode 1900/2000: avg_reward = -352.14, avg_steps=199.00 (over 100 episodes)\n",
      "[03:55:10] q_base.py:424 Episode 2000/2000: avg_reward = -354.80, avg_steps=199.00 (over 1000 episodes)\n",
      "CPU times: user 12min 20s, sys: 3.36 s, total: 12min 23s\n",
      "Wall time: 12min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "logging.ENV.debug_verbosity = 3\n",
    "policy = policy_impl.GreedyPolicyWithDecreasingRandomness(\n",
    "    initial_epsilon=1.0, final_epsilon=0.05, decay_by_half_after_num_of_episodes=500)\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:04:20] q_base.py:429 Episode 100/20000: avg_reward = -400.76, avg_steps=199.00 (over 100 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36mRun\u001b[0;34m(self, env, qfunc, policy, num_of_episodes)\u001b[0m\n\u001b[1;32m    379\u001b[0m           \u001b[0mqfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m           \u001b[0mtransition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtran\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           step_idx=step_idx)\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/runner_impl.py\u001b[0m in \u001b[0;36m_protected_ProcessTransition\u001b[0;34m(self, qfunc, transition, step_idx)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_every_n_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       qfunc.UpdateValues(\n\u001b[0;32m--> 103\u001b[0;31m         self._experience.Sample(self._experience_sample_batch_size))\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mGetHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36mUpdateValues\u001b[0;34m(self, transitions)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m# See: https://en.wikipedia.org/wiki/Q-learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;31m# axis=1 because action is always assumed to be 1-dimensional.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m     \u001b[0mnew_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdone_sp_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m       \u001b[0mnew_action_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2503\u001b[0m     \"\"\"\n\u001b[1;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[0;32m-> 2505\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "logging.ENV.debug_verbosity = 3\n",
    "policy = policy_impl.GreedyPolicyWithDecreasingRandomness(\n",
    "    initial_epsilon=1.0, final_epsilon=0.05, decay_by_half_after_num_of_episodes=500)\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train\n",
    "logging.ENV.debug_verbosity = 3\n",
    "policy = policy_impl.GreedyPolicyWithRandomness(epsilon=0.1)\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test\n",
    "logging.ENV.debug_verbosity = 4\n",
    "policy = policy_impl.GreedyPolicy()\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
