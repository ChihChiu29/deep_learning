{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "forwarded symbol: A\n",
      "forwarded symbol: T\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20-20-20\n",
    "Try the model that has shape (20, 20, 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # used in qfunc and runner.\n",
    "env = guided_environments.GuidedMountainCar(position_reward_factor=2.0, speed_reward_factor=5.0)\n",
    "qfunc = qfunc_impl.DQN(\n",
    "    model=qfunc_impl.CreateModel(\n",
    "        state_shape=env.GetStateShape(),\n",
    "        action_space_size=env.GetActionSpaceSize(),\n",
    "        hidden_layer_sizes=(20, 20, 20)),\n",
    "    training_batch_size=batch_size,\n",
    "    discount_factor=0.99,\n",
    ")\n",
    "runner = runner_impl.ExperienceReplayRunner(experience_capacity=100000, experience_sample_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:19:59] q_base.py:429 Episode 101/2000: averaged_episode_reward = -400.47, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[00:30:46] q_base.py:429 Episode 201/2000: averaged_episode_reward = -394.02, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[00:47:57] q_base.py:429 Episode 301/2000: averaged_episode_reward = -387.88, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[01:11:11] q_base.py:429 Episode 401/2000: averaged_episode_reward = -382.97, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[01:40:38] q_base.py:429 Episode 501/2000: averaged_episode_reward = -377.37, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[02:14:31] q_base.py:429 Episode 601/2000: averaged_episode_reward = -376.85, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[02:48:29] q_base.py:429 Episode 701/2000: averaged_episode_reward = -379.69, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[03:20:03] q_base.py:429 Episode 801/2000: averaged_episode_reward = -381.49, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[03:51:24] q_base.py:429 Episode 901/2000: averaged_episode_reward = -368.70, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[04:22:44] q_base.py:424 Episode 1001/2000: averaged_episode_reward = -381.26, averaged_steps=199.00 (averaged over 1000 episodes)\n",
      "[04:54:22] q_base.py:429 Episode 1101/2000: averaged_episode_reward = -361.64, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[05:26:57] q_base.py:429 Episode 1201/2000: averaged_episode_reward = -359.37, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[05:58:53] q_base.py:429 Episode 1301/2000: averaged_episode_reward = -358.29, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[06:30:49] q_base.py:429 Episode 1401/2000: averaged_episode_reward = -356.72, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[07:02:57] q_base.py:429 Episode 1501/2000: averaged_episode_reward = -355.15, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[07:36:27] q_base.py:429 Episode 1601/2000: averaged_episode_reward = -353.59, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[08:09:27] q_base.py:429 Episode 1701/2000: averaged_episode_reward = -352.12, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[08:42:47] q_base.py:429 Episode 1801/2000: averaged_episode_reward = -353.22, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[09:16:04] q_base.py:429 Episode 1901/2000: averaged_episode_reward = -350.27, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[09:49:11] q_base.py:424 Episode 2001/2000: averaged_episode_reward = -355.16, averaged_steps=199.00 (averaged over 1000 episodes)\n",
      "CPU times: user 9h 31min 18s, sys: 56.3 s, total: 9h 32min 14s\n",
      "Wall time: 9h 33min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "logging.ENV.debug_verbosity = 3\n",
    "policy = policy_impl.GreedyPolicyWithDecreasingRandomness(\n",
    "    initial_epsilon=1.0, final_epsilon=0.05, decay_by_half_after_num_of_episodes=500)\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:22:44] q_base.py:429 Episode 101/4000: averaged_episode_reward = -399.93, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[10:56:22] q_base.py:429 Episode 201/4000: averaged_episode_reward = -393.16, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[11:30:57] q_base.py:429 Episode 301/4000: averaged_episode_reward = -387.63, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[12:04:54] q_base.py:429 Episode 401/4000: averaged_episode_reward = -383.05, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[12:38:53] q_base.py:429 Episode 501/4000: averaged_episode_reward = -378.73, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[13:12:55] q_base.py:429 Episode 601/4000: averaged_episode_reward = -375.41, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[13:47:24] q_base.py:429 Episode 701/4000: averaged_episode_reward = -372.64, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[14:21:48] q_base.py:429 Episode 801/4000: averaged_episode_reward = -368.57, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[14:56:07] q_base.py:429 Episode 901/4000: averaged_episode_reward = -364.46, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[15:30:40] q_base.py:424 Episode 1001/4000: averaged_episode_reward = -378.59, averaged_steps=199.00 (averaged over 1000 episodes)\n",
      "[16:05:10] q_base.py:429 Episode 1101/4000: averaged_episode_reward = -360.68, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[16:39:20] q_base.py:429 Episode 1201/4000: averaged_episode_reward = -359.02, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[17:15:35] q_base.py:429 Episode 1301/4000: averaged_episode_reward = -356.79, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[17:51:56] q_base.py:429 Episode 1401/4000: averaged_episode_reward = -357.48, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[18:25:55] q_base.py:429 Episode 1501/4000: averaged_episode_reward = -355.48, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[18:59:41] q_base.py:429 Episode 1601/4000: averaged_episode_reward = -355.19, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[19:33:31] q_base.py:429 Episode 1701/4000: averaged_episode_reward = -353.26, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[20:07:19] q_base.py:429 Episode 1801/4000: averaged_episode_reward = -350.67, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[20:41:07] q_base.py:429 Episode 1901/4000: averaged_episode_reward = -351.41, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[21:14:59] q_base.py:424 Episode 2001/4000: averaged_episode_reward = -355.09, averaged_steps=199.00 (averaged over 1000 episodes)\n",
      "[21:49:18] q_base.py:429 Episode 2101/4000: averaged_episode_reward = -349.00, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[22:24:12] q_base.py:429 Episode 2201/4000: averaged_episode_reward = -350.29, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[23:01:18] q_base.py:429 Episode 2301/4000: averaged_episode_reward = -348.98, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[23:37:11] q_base.py:429 Episode 2401/4000: averaged_episode_reward = -347.23, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[00:10:33] q_base.py:429 Episode 2501/4000: averaged_episode_reward = -357.81, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[00:44:00] q_base.py:429 Episode 2601/4000: averaged_episode_reward = -368.21, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[01:17:31] q_base.py:429 Episode 2701/4000: averaged_episode_reward = -348.57, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[01:51:00] q_base.py:429 Episode 2801/4000: averaged_episode_reward = -350.10, averaged_steps=199.00 (averaged over 100 episodes)\n",
      "[02:25:36] q_base.py:429 Episode 2901/4000: averaged_episode_reward = -348.13, averaged_steps=199.00 (averaged over 100 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36mRun\u001b[0;34m(self, env, qfunc, policy, num_of_episodes)\u001b[0m\n\u001b[1;32m    379\u001b[0m           \u001b[0mqfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m           \u001b[0mtransition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtran\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           step_idx=step_idx)\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/runner_impl.py\u001b[0m in \u001b[0;36m_protected_ProcessTransition\u001b[0;34m(self, qfunc, transition, step_idx)\u001b[0m\n\u001b[1;32m     90\u001b[0m   def _protected_ProcessTransition(\n\u001b[1;32m     91\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0mqfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQFunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m       \u001b[0mtransition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m       \u001b[0mstep_idx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/runner_impl.py\u001b[0m in \u001b[0;36mSample\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     62\u001b[0m   ) -> None:\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m\"\"\"Adds an event to history.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capacity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2664\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prod_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2665\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train\n",
    "logging.ENV.debug_verbosity = 3\n",
    "policy = policy_impl.GreedyPolicyWithDecreasingRandomness(\n",
    "    initial_epsilon=1.0, final_epsilon=0.05, decay_by_half_after_num_of_episodes=500)\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train\n",
    "logging.ENV.debug_verbosity = 3\n",
    "policy = policy_impl.GreedyPolicyWithRandomness(epsilon=0.1)\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Test\n",
    "logging.ENV.debug_verbosity = 4\n",
    "policy = policy_impl.GreedyPolicy()\n",
    "runner.Run(env=env, qfunc=qfunc, policy=policy, num_of_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
