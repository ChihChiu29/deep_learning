{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_extension_impl\n",
      "forwarded symbol: runner_extension_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n",
      "reloaded: shortcut\n",
      "forwarded symbol: shortcut\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN_TargetNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:12:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[19:12:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[19:12:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='MsPacman-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x1ea670bbcc0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e93a15bdd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1e93a1847f0>,\n",
       " <keras.layers.core.Flatten at 0x1e93a19fef0>,\n",
       " <keras.layers.core.Dense at 0x1e93a1bb128>,\n",
       " <keras.layers.core.Dense at 0x1e93a1d0d30>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.qfunc._model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:19:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 10/3000: avg_reward = 216.00, avg_steps=631.70 (over 10 episodes)\n",
      "[19:24:36] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 20/3000: avg_reward = 197.00, avg_steps=644.80 (over 10 episodes)\n",
      "[19:31:20] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 30/3000: avg_reward = 292.00, avg_steps=702.10 (over 10 episodes)\n",
      "[19:38:29] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 40/3000: avg_reward = 285.00, avg_steps=665.70 (over 10 episodes)\n",
      "[19:46:23] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 50/3000: avg_reward = 273.00, avg_steps=643.40 (over 10 episodes)\n",
      "[19:56:21] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 60/3000: avg_reward = 302.00, avg_steps=696.30 (over 10 episodes)\n",
      "[20:07:38] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 70/3000: avg_reward = 346.00, avg_steps=669.10 (over 10 episodes)\n",
      "[20:19:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 80/3000: avg_reward = 248.00, avg_steps=632.00 (over 10 episodes)\n",
      "[20:34:13] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 90/3000: avg_reward = 387.00, avg_steps=679.70 (over 10 episodes)\n",
      "[20:54:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 100/3000: avg_reward = 373.00, avg_steps=718.40 (over 10 episodes)\n",
      "[21:11:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 110/3000: avg_reward = 318.00, avg_steps=641.70 (over 10 episodes)\n",
      "[21:29:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 120/3000: avg_reward = 404.00, avg_steps=635.60 (over 10 episodes)\n",
      "[21:50:07] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 130/3000: avg_reward = 299.00, avg_steps=690.90 (over 10 episodes)\n",
      "[22:12:11] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 140/3000: avg_reward = 376.00, avg_steps=695.90 (over 10 episodes)\n",
      "[22:35:11] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 150/3000: avg_reward = 338.00, avg_steps=671.20 (over 10 episodes)\n",
      "[23:09:13] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 160/3000: avg_reward = 333.00, avg_steps=670.90 (over 10 episodes)\n",
      "[23:44:11] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 170/3000: avg_reward = 436.00, avg_steps=712.40 (over 10 episodes)\n",
      "[00:14:55] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 180/3000: avg_reward = 379.00, avg_steps=640.90 (over 10 episodes)\n",
      "[00:51:25] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 190/3000: avg_reward = 519.00, avg_steps=784.70 (over 10 episodes)\n",
      "[01:22:36] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 200/3000: avg_reward = 389.00, avg_steps=696.20 (over 10 episodes)\n",
      "[01:52:09] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 210/3000: avg_reward = 339.00, avg_steps=678.30 (over 10 episodes)\n",
      "[02:24:23] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 220/3000: avg_reward = 457.00, avg_steps=765.30 (over 10 episodes)\n",
      "[02:53:41] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 230/3000: avg_reward = 398.00, avg_steps=721.40 (over 10 episodes)\n",
      "[03:26:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 240/3000: avg_reward = 480.00, avg_steps=842.20 (over 10 episodes)\n",
      "[03:51:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 250/3000: avg_reward = 366.00, avg_steps=678.70 (over 10 episodes)\n",
      "[04:20:38] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 260/3000: avg_reward = 551.00, avg_steps=798.00 (over 10 episodes)\n",
      "[04:45:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 270/3000: avg_reward = 555.00, avg_steps=737.90 (over 10 episodes)\n",
      "[05:06:49] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 280/3000: avg_reward = 370.00, avg_steps=645.50 (over 10 episodes)\n",
      "[05:30:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 290/3000: avg_reward = 496.00, avg_steps=747.30 (over 10 episodes)\n",
      "[05:51:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 300/3000: avg_reward = 401.00, avg_steps=694.50 (over 10 episodes)\n",
      "[06:16:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 310/3000: avg_reward = 529.00, avg_steps=778.70 (over 10 episodes)\n",
      "[06:43:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 320/3000: avg_reward = 654.00, avg_steps=853.90 (over 10 episodes)\n",
      "[07:07:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 330/3000: avg_reward = 587.00, avg_steps=737.30 (over 10 episodes)\n",
      "[07:30:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 340/3000: avg_reward = 480.00, avg_steps=708.30 (over 10 episodes)\n",
      "[07:56:29] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 350/3000: avg_reward = 646.00, avg_steps=802.00 (over 10 episodes)\n",
      "[08:19:31] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 360/3000: avg_reward = 449.00, avg_steps=711.30 (over 10 episodes)\n",
      "[08:44:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 370/3000: avg_reward = 522.00, avg_steps=780.80 (over 10 episodes)\n",
      "[09:07:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 380/3000: avg_reward = 549.00, avg_steps=702.10 (over 10 episodes)\n",
      "[09:33:53] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 390/3000: avg_reward = 573.00, avg_steps=814.40 (over 10 episodes)\n",
      "[09:56:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 400/3000: avg_reward = 500.00, avg_steps=712.50 (over 10 episodes)\n",
      "[10:17:59] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 410/3000: avg_reward = 580.00, avg_steps=695.10 (over 10 episodes)\n",
      "[10:43:38] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 420/3000: avg_reward = 831.00, avg_steps=825.90 (over 10 episodes)\n",
      "[11:04:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 430/3000: avg_reward = 461.00, avg_steps=693.30 (over 10 episodes)\n",
      "[11:27:15] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 440/3000: avg_reward = 391.00, avg_steps=732.10 (over 10 episodes)\n",
      "[11:49:09] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 450/3000: avg_reward = 451.00, avg_steps=721.90 (over 10 episodes)\n",
      "[12:16:38] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 460/3000: avg_reward = 876.00, avg_steps=905.20 (over 10 episodes)\n",
      "[12:37:50] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 470/3000: avg_reward = 524.00, avg_steps=704.20 (over 10 episodes)\n",
      "[12:59:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 480/3000: avg_reward = 506.00, avg_steps=719.90 (over 10 episodes)\n",
      "[13:21:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 490/3000: avg_reward = 547.00, avg_steps=744.80 (over 10 episodes)\n",
      "[13:47:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 500/3000: avg_reward = 650.00, avg_steps=865.40 (over 10 episodes)\n",
      "[14:06:21] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 510/3000: avg_reward = 469.00, avg_steps=648.00 (over 10 episodes)\n",
      "[14:28:47] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 520/3000: avg_reward = 622.00, avg_steps=763.80 (over 10 episodes)\n",
      "[14:51:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 530/3000: avg_reward = 575.00, avg_steps=782.80 (over 10 episodes)\n",
      "[15:10:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 540/3000: avg_reward = 419.00, avg_steps=662.80 (over 10 episodes)\n",
      "[15:35:44] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 550/3000: avg_reward = 769.00, avg_steps=875.00 (over 10 episodes)\n",
      "[15:58:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 560/3000: avg_reward = 627.00, avg_steps=756.70 (over 10 episodes)\n",
      "[16:21:01] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 570/3000: avg_reward = 652.00, avg_steps=722.80 (over 10 episodes)\n",
      "[16:44:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 580/3000: avg_reward = 501.00, avg_steps=770.90 (over 10 episodes)\n",
      "[17:14:06] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 590/3000: avg_reward = 612.00, avg_steps=747.50 (over 10 episodes)\n",
      "[17:52:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 600/3000: avg_reward = 569.00, avg_steps=811.30 (over 10 episodes)\n",
      "[18:33:15] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 610/3000: avg_reward = 751.00, avg_steps=738.40 (over 10 episodes)\n",
      "[19:12:09] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 620/3000: avg_reward = 659.00, avg_steps=846.10 (over 10 episodes)\n",
      "[19:40:13] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 630/3000: avg_reward = 538.00, avg_steps=782.30 (over 10 episodes)\n",
      "[20:05:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 640/3000: avg_reward = 658.00, avg_steps=748.30 (over 10 episodes)\n",
      "[20:30:37] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 650/3000: avg_reward = 729.00, avg_steps=767.00 (over 10 episodes)\n",
      "[21:09:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 660/3000: avg_reward = 764.00, avg_steps=716.50 (over 10 episodes)\n",
      "[21:47:53] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 670/3000: avg_reward = 763.00, avg_steps=839.50 (over 10 episodes)\n",
      "[22:21:05] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 680/3000: avg_reward = 528.00, avg_steps=717.10 (over 10 episodes)\n",
      "[22:57:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 690/3000: avg_reward = 559.00, avg_steps=820.40 (over 10 episodes)\n",
      "[23:36:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 700/3000: avg_reward = 867.00, avg_steps=873.60 (over 10 episodes)\n",
      "[00:11:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 710/3000: avg_reward = 530.00, avg_steps=781.70 (over 10 episodes)\n",
      "[00:43:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 720/3000: avg_reward = 483.00, avg_steps=735.60 (over 10 episodes)\n",
      "[01:10:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 730/3000: avg_reward = 437.00, avg_steps=613.40 (over 10 episodes)\n",
      "[01:45:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 740/3000: avg_reward = 573.00, avg_steps=795.00 (over 10 episodes)\n",
      "[02:22:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 750/3000: avg_reward = 544.00, avg_steps=849.80 (over 10 episodes)\n",
      "[02:56:32] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 760/3000: avg_reward = 689.00, avg_steps=785.50 (over 10 episodes)\n",
      "[03:33:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 770/3000: avg_reward = 828.00, avg_steps=837.90 (over 10 episodes)\n",
      "[04:06:29] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 780/3000: avg_reward = 767.00, avg_steps=768.60 (over 10 episodes)\n",
      "[04:36:04] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 790/3000: avg_reward = 639.00, avg_steps=678.80 (over 10 episodes)\n",
      "[05:09:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 800/3000: avg_reward = 678.00, avg_steps=784.20 (over 10 episodes)\n",
      "[05:43:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 810/3000: avg_reward = 975.00, avg_steps=772.20 (over 10 episodes)\n",
      "[06:19:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 820/3000: avg_reward = 686.00, avg_steps=841.30 (over 10 episodes)\n",
      "[07:16:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 830/3000: avg_reward = 598.00, avg_steps=710.00 (over 10 episodes)\n",
      "[08:28:29] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 840/3000: avg_reward = 644.00, avg_steps=781.50 (over 10 episodes)\n",
      "[09:34:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 850/3000: avg_reward = 579.00, avg_steps=732.60 (over 10 episodes)\n",
      "[10:52:13] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 860/3000: avg_reward = 666.00, avg_steps=868.60 (over 10 episodes)\n",
      "[12:03:07] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 870/3000: avg_reward = 733.00, avg_steps=803.10 (over 10 episodes)\n",
      "[13:22:53] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 880/3000: avg_reward = 796.00, avg_steps=902.40 (over 10 episodes)\n",
      "[14:34:50] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 890/3000: avg_reward = 608.00, avg_steps=826.50 (over 10 episodes)\n",
      "[15:37:12] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 900/3000: avg_reward = 557.00, avg_steps=710.60 (over 10 episodes)\n",
      "[16:40:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 910/3000: avg_reward = 557.00, avg_steps=722.80 (over 10 episodes)\n",
      "[17:37:58] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 920/3000: avg_reward = 360.00, avg_steps=662.70 (over 10 episodes)\n",
      "[18:32:58] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 930/3000: avg_reward = 386.00, avg_steps=661.20 (over 10 episodes)\n",
      "[19:00:36] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 940/3000: avg_reward = 614.00, avg_steps=767.60 (over 10 episodes)\n",
      "[19:29:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 950/3000: avg_reward = 621.00, avg_steps=778.40 (over 10 episodes)\n",
      "[20:01:58] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 960/3000: avg_reward = 571.00, avg_steps=788.10 (over 10 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-216a1173130b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\examples\\shortcut.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, num_of_episodes)\u001b[0m\n\u001b[0;32m    198\u001b[0m     self._progress_tracer = runner_extension_impl.ProgressTracer(\n\u001b[0;32m    199\u001b[0m       report_every_num_of_episodes=report_every_num_of_episodes)\n\u001b[1;32m--> 200\u001b[1;33m     self._model_saver = runner_extension_impl.ModelSaver(\n\u001b[0m\u001b[0;32m    201\u001b[0m       self._GetModelWeightsFilepath())\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mRun\u001b[1;34m(self, env, qfunc, policy, num_of_episodes)\u001b[0m\n\u001b[0;32m    429\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Running episode: %d, step: %d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisode_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         tran = env.TakeAction(\n\u001b[0m\u001b[0;32m    432\u001b[0m           policy.Decide(\n\u001b[0;32m    433\u001b[0m             \u001b[0menv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\runner_impl.py\u001b[0m in \u001b[0;36m_protected_ProcessTransition\u001b[1;34m(self, qfunc, transition, step_idx)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_every_n_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m       qfunc.UpdateValues(\n\u001b[1;32m---> 99\u001b[1;33m         self._experience.Sample(self._experience_sample_batch_size))\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSampleFromHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mUpdateValues\u001b[1;34m(self, transitions)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[0mnew_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdone_sp_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m       \u001b[0mnew_action_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m     \u001b[0mlearn_new_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnew_action_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36m_SetActionValues\u001b[1;34m(self, states, actions, action_values, values)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mnumbers\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mall\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mequal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m       \u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstates\u001b[0m \u001b[0mto\u001b[0m \u001b[0mset\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m       \u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactions\u001b[0m \u001b[0mto\u001b[0m \u001b[0mset\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mGetValues\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;34m\"\"\"Saves (maybe partially) to a file.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\qfunc_impl.py\u001b[0m in \u001b[0;36m_protected_GetValues\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m    228\u001b[0m       \u001b[0mstates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mq_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m   ) -> q_base.QValues:\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m   \u001b[1;31m# @Override\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline.Train(num_of_episodes=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DDQN\n",
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='MsPacman-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:42:37] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 10/3000: avg_reward = 199.00, avg_steps=657.10 (over 10 episodes)\n",
      "[20:48:49] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 20/3000: avg_reward = 243.00, avg_steps=669.10 (over 10 episodes)\n",
      "[20:54:36] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 30/3000: avg_reward = 207.00, avg_steps=604.80 (over 10 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-216a1173130b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\examples\\shortcut.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, num_of_episodes)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m       \u001b[0mnum_of_episodes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mruns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \"\"\"\n\u001b[0;32m    219\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClearCallbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mRun\u001b[1;34m(self, env, qfunc, policy, num_of_episodes)\u001b[0m\n\u001b[0;32m    441\u001b[0m           \u001b[0mqfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m           \u001b[0mtransition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtran\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m           step_idx=step_idx)\n\u001b[0m\u001b[0;32m    444\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtran\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtran\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\runner_impl.py\u001b[0m in \u001b[0;36m_protected_ProcessTransition\u001b[1;34m(self, qfunc, transition, step_idx)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_every_n_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m       qfunc.UpdateValues(\n\u001b[1;32m---> 99\u001b[1;33m         self._experience.Sample(self._experience_sample_batch_size))\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSampleFromHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\qfunc_impl.py\u001b[0m in \u001b[0;36mUpdateValues\u001b[1;34m(self, transitions)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mdone_sp_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     states, actions, rewards, new_states = (\n\u001b[1;32m--> 340\u001b[1;33m       \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m       \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline.Train(num_of_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
