{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path c:\\Workspace\\git\\ added to PYTHON_PATH.\n",
      "Path c:\\Workspace\\git\\deep_learning added to PYTHON_PATH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_extension_impl\n",
      "forwarded symbol: runner_extension_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n",
      "reloaded: running_environment\n",
      "forwarded symbol: running_environment\n",
      "reloaded: shortcut\n",
      "forwarded symbol: shortcut\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_environment.CheckAndForceGpuForTheRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN_TargetNetwork Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Workspace\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[09:36:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[09:36:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[09:36:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='SpaceInvaders-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    "    use_ddqn=False,\n",
    "    use_large_model=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Workspace\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[09:37:03] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 130.0\n",
      "[09:38:02] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 135.0\n",
      "[09:38:22] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 135.0\n",
      "[09:40:00] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 161.66666666666666\n",
      "[09:40:21] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 10/1500: avg_reward = 153.50, avg_steps=702.70 (over 10 episodes)\n",
      "[09:42:47] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 188.5\n",
      "[09:43:16] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 196.5\n",
      "[09:44:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 20/1500: avg_reward = 177.50, avg_steps=726.30 (over 10 episodes)\n",
      "[09:45:04] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 198.5\n",
      "[09:46:04] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 200.5\n",
      "[09:48:59] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 207.5\n",
      "[09:49:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 30/1500: avg_reward = 208.50, avg_steps=783.90 (over 10 episodes)\n",
      "[09:49:42] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 208.5\n",
      "[09:51:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 225.5\n",
      "[09:52:18] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 226.5\n",
      "[09:54:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 40/1500: avg_reward = 151.00, avg_steps=671.40 (over 10 episodes)\n",
      "[10:01:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 50/1500: avg_reward = 183.00, avg_steps=835.40 (over 10 episodes)\n",
      "[10:06:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 60/1500: avg_reward = 113.50, avg_steps=571.70 (over 10 episodes)\n",
      "[10:15:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 70/1500: avg_reward = 202.00, avg_steps=881.00 (over 10 episodes)\n",
      "[10:18:34] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 246.5\n",
      "[10:23:27] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 247.5\n",
      "[10:25:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 80/1500: avg_reward = 236.50, avg_steps=825.50 (over 10 episodes)\n",
      "[10:27:30] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 268.5\n",
      "[10:29:29] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 274.0\n",
      "[10:36:31] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 90/1500: avg_reward = 197.50, avg_steps=756.00 (over 10 episodes)\n",
      "[10:49:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 100/1500: avg_reward = 210.00, avg_steps=732.10 (over 10 episodes)\n",
      "[11:05:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 110/1500: avg_reward = 194.00, avg_steps=782.30 (over 10 episodes)\n",
      "[11:23:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 120/1500: avg_reward = 240.00, avg_steps=785.60 (over 10 episodes)\n",
      "[11:43:07] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 130/1500: avg_reward = 220.00, avg_steps=752.60 (over 10 episodes)\n",
      "[12:10:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 140/1500: avg_reward = 172.00, avg_steps=740.10 (over 10 episodes)\n",
      "[12:42:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 150/1500: avg_reward = 247.00, avg_steps=825.20 (over 10 episodes)\n",
      "[13:09:05] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 160/1500: avg_reward = 184.50, avg_steps=707.50 (over 10 episodes)\n",
      "[13:36:57] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 170/1500: avg_reward = 184.50, avg_steps=763.70 (over 10 episodes)\n",
      "[14:04:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 180/1500: avg_reward = 246.00, avg_steps=781.90 (over 10 episodes)\n",
      "[14:29:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 190/1500: avg_reward = 174.50, avg_steps=764.40 (over 10 episodes)\n",
      "[14:55:00] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 200/1500: avg_reward = 209.50, avg_steps=832.00 (over 10 episodes)\n",
      "[15:10:48] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 210/1500: avg_reward = 89.00, avg_steps=543.70 (over 10 episodes)\n",
      "[15:32:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 220/1500: avg_reward = 214.50, avg_steps=806.50 (over 10 episodes)\n",
      "[15:38:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 303.5\n",
      "[15:51:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 230/1500: avg_reward = 239.50, avg_steps=727.70 (over 10 episodes)\n",
      "[16:07:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 240/1500: avg_reward = 155.00, avg_steps=722.50 (over 10 episodes)\n",
      "[16:22:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 250/1500: avg_reward = 194.00, avg_steps=682.40 (over 10 episodes)\n",
      "[16:30:20] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 315.0\n",
      "[16:32:39] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 353.0\n",
      "[16:41:51] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 260/1500: avg_reward = 294.50, avg_steps=992.90 (over 10 episodes)\n",
      "[16:54:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 270/1500: avg_reward = 147.50, avg_steps=675.00 (over 10 episodes)\n",
      "[17:08:47] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 280/1500: avg_reward = 180.50, avg_steps=768.20 (over 10 episodes)\n",
      "[17:23:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 290/1500: avg_reward = 192.00, avg_steps=766.50 (over 10 episodes)\n",
      "[17:38:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 300/1500: avg_reward = 236.50, avg_steps=796.60 (over 10 episodes)\n",
      "[17:54:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 310/1500: avg_reward = 255.00, avg_steps=850.00 (over 10 episodes)\n",
      "[18:11:07] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 320/1500: avg_reward = 221.00, avg_steps=849.80 (over 10 episodes)\n",
      "[18:34:55] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 330/1500: avg_reward = 225.50, avg_steps=805.50 (over 10 episodes)\n",
      "[18:59:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 340/1500: avg_reward = 187.50, avg_steps=738.00 (over 10 episodes)\n",
      "[19:23:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 350/1500: avg_reward = 184.50, avg_steps=717.60 (over 10 episodes)\n",
      "[19:41:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 360/1500: avg_reward = 150.00, avg_steps=665.10 (over 10 episodes)\n",
      "[20:07:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 370/1500: avg_reward = 244.50, avg_steps=985.00 (over 10 episodes)\n",
      "[20:29:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 380/1500: avg_reward = 186.50, avg_steps=813.20 (over 10 episodes)\n",
      "[20:50:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 390/1500: avg_reward = 176.50, avg_steps=837.60 (over 10 episodes)\n",
      "[21:08:58] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 400/1500: avg_reward = 173.00, avg_steps=758.30 (over 10 episodes)\n",
      "[21:26:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 410/1500: avg_reward = 177.50, avg_steps=716.60 (over 10 episodes)\n",
      "[21:44:20] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 420/1500: avg_reward = 205.50, avg_steps=740.30 (over 10 episodes)\n",
      "[22:02:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 430/1500: avg_reward = 206.00, avg_steps=771.40 (over 10 episodes)\n",
      "[22:18:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 440/1500: avg_reward = 185.00, avg_steps=738.10 (over 10 episodes)\n",
      "[22:36:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 450/1500: avg_reward = 183.50, avg_steps=692.60 (over 10 episodes)\n"
     ]
    }
   ],
   "source": [
    "logging.ENV.debug_verbosity = 6\n",
    "pipeline.Train(num_of_episodes=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN_TargetNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:59:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[20:59:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[20:59:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='SpaceInvaders-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    "    use_ddqn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Workspace\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[21:04:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 10/3000: avg_reward = 200.50, avg_steps=797.50 (over 10 episodes)\n",
      "[21:09:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 20/3000: avg_reward = 142.50, avg_steps=696.90 (over 10 episodes)\n",
      "[21:14:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 30/3000: avg_reward = 101.00, avg_steps=613.00 (over 10 episodes)\n",
      "[21:19:31] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 40/3000: avg_reward = 117.50, avg_steps=621.80 (over 10 episodes)\n",
      "[21:27:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 50/3000: avg_reward = 195.00, avg_steps=805.20 (over 10 episodes)\n",
      "[21:34:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 60/3000: avg_reward = 175.50, avg_steps=708.90 (over 10 episodes)\n",
      "[21:45:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 70/3000: avg_reward = 196.00, avg_steps=802.30 (over 10 episodes)\n",
      "[21:56:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 80/3000: avg_reward = 147.00, avg_steps=748.50 (over 10 episodes)\n",
      "[22:12:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 90/3000: avg_reward = 189.50, avg_steps=849.50 (over 10 episodes)\n",
      "[22:31:11] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 100/3000: avg_reward = 200.00, avg_steps=854.30 (over 10 episodes)\n",
      "[22:50:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 110/3000: avg_reward = 228.00, avg_steps=768.40 (over 10 episodes)\n",
      "[23:08:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 120/3000: avg_reward = 126.00, avg_steps=634.30 (over 10 episodes)\n",
      "[23:26:06] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 130/3000: avg_reward = 145.00, avg_steps=590.70 (over 10 episodes)\n",
      "[23:56:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 140/3000: avg_reward = 179.50, avg_steps=788.40 (over 10 episodes)\n",
      "[00:32:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 150/3000: avg_reward = 168.50, avg_steps=783.70 (over 10 episodes)\n",
      "[01:03:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 160/3000: avg_reward = 181.00, avg_steps=692.20 (over 10 episodes)\n",
      "[01:37:37] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 170/3000: avg_reward = 215.50, avg_steps=778.40 (over 10 episodes)\n",
      "[02:05:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 180/3000: avg_reward = 160.50, avg_steps=673.00 (over 10 episodes)\n",
      "[02:36:50] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 190/3000: avg_reward = 281.00, avg_steps=780.60 (over 10 episodes)\n",
      "[03:12:00] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 200/3000: avg_reward = 289.50, avg_steps=938.50 (over 10 episodes)\n",
      "[03:37:20] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 210/3000: avg_reward = 162.00, avg_steps=715.00 (over 10 episodes)\n",
      "[04:00:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 220/3000: avg_reward = 186.50, avg_steps=683.20 (over 10 episodes)\n",
      "[04:24:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 230/3000: avg_reward = 168.00, avg_steps=775.20 (over 10 episodes)\n",
      "[04:45:01] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 240/3000: avg_reward = 177.50, avg_steps=686.10 (over 10 episodes)\n",
      "[05:05:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 250/3000: avg_reward = 193.50, avg_steps=743.40 (over 10 episodes)\n",
      "[05:24:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 260/3000: avg_reward = 185.50, avg_steps=765.10 (over 10 episodes)\n",
      "[05:42:37] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 270/3000: avg_reward = 178.00, avg_steps=754.40 (over 10 episodes)\n",
      "[06:00:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 280/3000: avg_reward = 179.00, avg_steps=743.30 (over 10 episodes)\n",
      "[06:18:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 290/3000: avg_reward = 179.50, avg_steps=715.00 (over 10 episodes)\n",
      "[06:35:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 300/3000: avg_reward = 167.00, avg_steps=712.30 (over 10 episodes)\n",
      "[06:51:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 310/3000: avg_reward = 125.00, avg_steps=611.40 (over 10 episodes)\n",
      "[07:09:04] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 320/3000: avg_reward = 155.00, avg_steps=727.20 (over 10 episodes)\n",
      "[07:28:41] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 330/3000: avg_reward = 219.00, avg_steps=806.50 (over 10 episodes)\n",
      "[07:43:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 340/3000: avg_reward = 167.00, avg_steps=605.60 (over 10 episodes)\n",
      "[08:00:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 350/3000: avg_reward = 147.00, avg_steps=677.00 (over 10 episodes)\n",
      "[08:14:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 360/3000: avg_reward = 134.50, avg_steps=600.60 (over 10 episodes)\n",
      "[08:38:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 370/3000: avg_reward = 259.50, avg_steps=969.00 (over 10 episodes)\n",
      "[08:55:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 380/3000: avg_reward = 155.00, avg_steps=693.30 (over 10 episodes)\n",
      "[09:13:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 390/3000: avg_reward = 273.00, avg_steps=790.50 (over 10 episodes)\n",
      "[09:34:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 400/3000: avg_reward = 240.00, avg_steps=893.60 (over 10 episodes)\n",
      "[09:53:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 410/3000: avg_reward = 195.00, avg_steps=808.40 (over 10 episodes)\n",
      "[10:11:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 420/3000: avg_reward = 207.00, avg_steps=760.60 (over 10 episodes)\n",
      "[10:29:26] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 430/3000: avg_reward = 202.50, avg_steps=808.40 (over 10 episodes)\n",
      "[10:47:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 440/3000: avg_reward = 210.00, avg_steps=783.80 (over 10 episodes)\n",
      "[11:07:45] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 450/3000: avg_reward = 254.50, avg_steps=931.40 (over 10 episodes)\n",
      "[11:23:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 460/3000: avg_reward = 177.50, avg_steps=722.80 (over 10 episodes)\n",
      "[11:38:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 470/3000: avg_reward = 157.50, avg_steps=693.20 (over 10 episodes)\n",
      "[11:56:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 480/3000: avg_reward = 217.00, avg_steps=820.70 (over 10 episodes)\n",
      "[12:16:47] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 490/3000: avg_reward = 223.50, avg_steps=929.10 (over 10 episodes)\n",
      "[12:30:44] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 500/3000: avg_reward = 137.00, avg_steps=647.50 (over 10 episodes)\n",
      "[12:44:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 510/3000: avg_reward = 160.00, avg_steps=657.50 (over 10 episodes)\n",
      "[12:59:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 520/3000: avg_reward = 237.50, avg_steps=694.00 (over 10 episodes)\n",
      "[13:16:51] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 530/3000: avg_reward = 232.00, avg_steps=814.90 (over 10 episodes)\n",
      "[13:32:32] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 540/3000: avg_reward = 163.00, avg_steps=745.30 (over 10 episodes)\n",
      "[13:50:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 550/3000: avg_reward = 264.00, avg_steps=831.30 (over 10 episodes)\n",
      "[14:02:23] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 560/3000: avg_reward = 137.00, avg_steps=592.00 (over 10 episodes)\n",
      "[14:16:50] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 570/3000: avg_reward = 159.00, avg_steps=697.50 (over 10 episodes)\n",
      "[14:30:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 580/3000: avg_reward = 168.00, avg_steps=662.90 (over 10 episodes)\n",
      "[14:45:53] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 590/3000: avg_reward = 246.50, avg_steps=778.00 (over 10 episodes)\n",
      "[14:59:44] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 600/3000: avg_reward = 150.50, avg_steps=700.20 (over 10 episodes)\n",
      "[15:11:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 610/3000: avg_reward = 136.50, avg_steps=604.70 (over 10 episodes)\n",
      "[15:28:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 620/3000: avg_reward = 235.00, avg_steps=838.40 (over 10 episodes)\n",
      "[15:43:00] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 630/3000: avg_reward = 229.00, avg_steps=755.70 (over 10 episodes)\n",
      "[15:58:13] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 640/3000: avg_reward = 230.50, avg_steps=778.50 (over 10 episodes)\n",
      "[16:12:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 650/3000: avg_reward = 148.00, avg_steps=766.60 (over 10 episodes)\n",
      "[16:28:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 660/3000: avg_reward = 186.00, avg_steps=824.30 (over 10 episodes)\n",
      "[16:40:45] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 670/3000: avg_reward = 139.00, avg_steps=642.90 (over 10 episodes)\n",
      "[16:54:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 680/3000: avg_reward = 175.00, avg_steps=712.80 (over 10 episodes)\n",
      "[17:06:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 690/3000: avg_reward = 140.00, avg_steps=664.60 (over 10 episodes)\n",
      "[17:21:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 700/3000: avg_reward = 214.50, avg_steps=768.30 (over 10 episodes)\n",
      "[17:33:01] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 710/3000: avg_reward = 161.00, avg_steps=622.80 (over 10 episodes)\n",
      "[17:44:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 720/3000: avg_reward = 143.50, avg_steps=633.50 (over 10 episodes)\n",
      "[17:56:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 730/3000: avg_reward = 127.50, avg_steps=681.20 (over 10 episodes)\n"
     ]
    }
   ],
   "source": [
    "pipeline.Train(num_of_episodes=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DDQN\n",
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='SpaceInvaders-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    "    use_ddqn=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.Train(num_of_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
