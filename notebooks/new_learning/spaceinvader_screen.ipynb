{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path c:\\Workspace\\git\\ added to PYTHON_PATH.\n",
      "Path c:\\Workspace\\git\\deep_learning added to PYTHON_PATH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_extension_impl\n",
      "forwarded symbol: runner_extension_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n",
      "reloaded: running_environment\n",
      "forwarded symbol: running_environment\n",
      "reloaded: shortcut\n",
      "forwarded symbol: shortcut\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_environment.CheckAndForceGpuForTheRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN_TargetNetwork Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Workspace\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "[09:36:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[09:36:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[09:36:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='SpaceInvaders-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    "    use_ddqn=False,\n",
    "    use_large_model=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Workspace\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[09:37:03] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 130.0\n",
      "[09:38:02] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 135.0\n",
      "[09:38:22] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 135.0\n",
      "[09:40:00] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 161.66666666666666\n",
      "[09:40:21] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 10/1500: avg_reward = 153.50, avg_steps=702.70 (over 10 episodes)\n",
      "[09:42:47] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 188.5\n",
      "[09:43:16] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 196.5\n",
      "[09:44:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 20/1500: avg_reward = 177.50, avg_steps=726.30 (over 10 episodes)\n",
      "[09:45:04] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 198.5\n",
      "[09:46:04] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 200.5\n",
      "[09:48:59] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 207.5\n",
      "[09:49:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 30/1500: avg_reward = 208.50, avg_steps=783.90 (over 10 episodes)\n",
      "[09:49:42] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 208.5\n",
      "[09:51:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 225.5\n",
      "[09:52:18] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 226.5\n",
      "[09:54:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 40/1500: avg_reward = 151.00, avg_steps=671.40 (over 10 episodes)\n",
      "[10:01:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 50/1500: avg_reward = 183.00, avg_steps=835.40 (over 10 episodes)\n",
      "[10:06:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 60/1500: avg_reward = 113.50, avg_steps=571.70 (over 10 episodes)\n",
      "[10:15:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 70/1500: avg_reward = 202.00, avg_steps=881.00 (over 10 episodes)\n",
      "[10:18:34] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 246.5\n",
      "[10:23:27] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 247.5\n",
      "[10:25:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 80/1500: avg_reward = 236.50, avg_steps=825.50 (over 10 episodes)\n",
      "[10:27:30] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 268.5\n",
      "[10:29:29] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 274.0\n",
      "[10:36:31] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 90/1500: avg_reward = 197.50, avg_steps=756.00 (over 10 episodes)\n",
      "[10:49:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 100/1500: avg_reward = 210.00, avg_steps=732.10 (over 10 episodes)\n",
      "[11:05:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 110/1500: avg_reward = 194.00, avg_steps=782.30 (over 10 episodes)\n",
      "[11:23:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 120/1500: avg_reward = 240.00, avg_steps=785.60 (over 10 episodes)\n",
      "[11:43:07] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 130/1500: avg_reward = 220.00, avg_steps=752.60 (over 10 episodes)\n",
      "[12:10:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 140/1500: avg_reward = 172.00, avg_steps=740.10 (over 10 episodes)\n",
      "[12:42:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 150/1500: avg_reward = 247.00, avg_steps=825.20 (over 10 episodes)\n",
      "[13:09:05] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 160/1500: avg_reward = 184.50, avg_steps=707.50 (over 10 episodes)\n",
      "[13:36:57] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 170/1500: avg_reward = 184.50, avg_steps=763.70 (over 10 episodes)\n",
      "[14:04:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 180/1500: avg_reward = 246.00, avg_steps=781.90 (over 10 episodes)\n",
      "[14:29:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 190/1500: avg_reward = 174.50, avg_steps=764.40 (over 10 episodes)\n",
      "[14:55:00] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 200/1500: avg_reward = 209.50, avg_steps=832.00 (over 10 episodes)\n",
      "[15:10:48] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 210/1500: avg_reward = 89.00, avg_steps=543.70 (over 10 episodes)\n",
      "[15:32:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 220/1500: avg_reward = 214.50, avg_steps=806.50 (over 10 episodes)\n",
      "[15:38:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 303.5\n",
      "[15:51:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 230/1500: avg_reward = 239.50, avg_steps=727.70 (over 10 episodes)\n",
      "[16:07:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 240/1500: avg_reward = 155.00, avg_steps=722.50 (over 10 episodes)\n",
      "[16:22:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 250/1500: avg_reward = 194.00, avg_steps=682.40 (over 10 episodes)\n",
      "[16:30:20] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 315.0\n",
      "[16:32:39] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:195 saving model for new best value: 353.0\n",
      "[16:41:51] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 260/1500: avg_reward = 294.50, avg_steps=992.90 (over 10 episodes)\n",
      "[16:54:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 270/1500: avg_reward = 147.50, avg_steps=675.00 (over 10 episodes)\n",
      "[17:08:47] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 280/1500: avg_reward = 180.50, avg_steps=768.20 (over 10 episodes)\n",
      "[17:23:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 290/1500: avg_reward = 192.00, avg_steps=766.50 (over 10 episodes)\n",
      "[17:38:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 300/1500: avg_reward = 236.50, avg_steps=796.60 (over 10 episodes)\n",
      "[17:54:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 310/1500: avg_reward = 255.00, avg_steps=850.00 (over 10 episodes)\n",
      "[18:11:07] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 320/1500: avg_reward = 221.00, avg_steps=849.80 (over 10 episodes)\n",
      "[18:34:55] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 330/1500: avg_reward = 225.50, avg_steps=805.50 (over 10 episodes)\n",
      "[18:59:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 340/1500: avg_reward = 187.50, avg_steps=738.00 (over 10 episodes)\n",
      "[19:23:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 350/1500: avg_reward = 184.50, avg_steps=717.60 (over 10 episodes)\n",
      "[19:41:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 360/1500: avg_reward = 150.00, avg_steps=665.10 (over 10 episodes)\n",
      "[20:07:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 370/1500: avg_reward = 244.50, avg_steps=985.00 (over 10 episodes)\n",
      "[20:29:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 380/1500: avg_reward = 186.50, avg_steps=813.20 (over 10 episodes)\n",
      "[20:50:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 390/1500: avg_reward = 176.50, avg_steps=837.60 (over 10 episodes)\n",
      "[21:08:58] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 400/1500: avg_reward = 173.00, avg_steps=758.30 (over 10 episodes)\n",
      "[21:26:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 410/1500: avg_reward = 177.50, avg_steps=716.60 (over 10 episodes)\n",
      "[21:44:20] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 420/1500: avg_reward = 205.50, avg_steps=740.30 (over 10 episodes)\n",
      "[22:02:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 430/1500: avg_reward = 206.00, avg_steps=771.40 (over 10 episodes)\n",
      "[22:18:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 440/1500: avg_reward = 185.00, avg_steps=738.10 (over 10 episodes)\n",
      "[22:36:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 450/1500: avg_reward = 183.50, avg_steps=692.60 (over 10 episodes)\n",
      "[22:55:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 460/1500: avg_reward = 161.50, avg_steps=723.90 (over 10 episodes)\n",
      "[23:16:41] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 470/1500: avg_reward = 164.00, avg_steps=690.20 (over 10 episodes)\n",
      "[23:40:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 480/1500: avg_reward = 288.50, avg_steps=950.20 (over 10 episodes)\n",
      "[00:01:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 490/1500: avg_reward = 204.50, avg_steps=854.20 (over 10 episodes)\n",
      "[00:17:55] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 500/1500: avg_reward = 164.00, avg_steps=762.50 (over 10 episodes)\n",
      "[00:34:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 510/1500: avg_reward = 208.00, avg_steps=767.70 (over 10 episodes)\n",
      "[00:53:09] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 520/1500: avg_reward = 268.50, avg_steps=874.70 (over 10 episodes)\n",
      "[01:10:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 530/1500: avg_reward = 223.00, avg_steps=845.80 (over 10 episodes)\n",
      "[01:26:23] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 540/1500: avg_reward = 178.00, avg_steps=741.00 (over 10 episodes)\n",
      "[01:42:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 550/1500: avg_reward = 214.50, avg_steps=789.10 (over 10 episodes)\n",
      "[01:57:38] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 560/1500: avg_reward = 199.00, avg_steps=772.90 (over 10 episodes)\n",
      "[02:13:01] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 570/1500: avg_reward = 191.50, avg_steps=782.80 (over 10 episodes)\n",
      "[02:26:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 580/1500: avg_reward = 166.00, avg_steps=694.60 (over 10 episodes)\n",
      "[02:41:48] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 590/1500: avg_reward = 232.00, avg_steps=778.30 (over 10 episodes)\n",
      "[02:58:47] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 600/1500: avg_reward = 229.00, avg_steps=887.20 (over 10 episodes)\n",
      "[03:15:25] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 610/1500: avg_reward = 256.50, avg_steps=877.60 (over 10 episodes)\n",
      "[03:31:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 620/1500: avg_reward = 263.50, avg_steps=836.20 (over 10 episodes)\n",
      "[03:45:26] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 630/1500: avg_reward = 199.00, avg_steps=753.70 (over 10 episodes)\n",
      "[04:02:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 640/1500: avg_reward = 204.50, avg_steps=890.70 (over 10 episodes)\n",
      "[04:14:32] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 650/1500: avg_reward = 184.50, avg_steps=659.10 (over 10 episodes)\n",
      "[04:30:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 660/1500: avg_reward = 290.50, avg_steps=883.70 (over 10 episodes)\n",
      "[04:43:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 670/1500: avg_reward = 167.00, avg_steps=686.00 (over 10 episodes)\n",
      "[04:59:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 680/1500: avg_reward = 247.00, avg_steps=854.00 (over 10 episodes)\n",
      "[05:13:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 690/1500: avg_reward = 174.00, avg_steps=788.50 (over 10 episodes)\n",
      "[05:28:55] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 700/1500: avg_reward = 239.50, avg_steps=875.20 (over 10 episodes)\n",
      "[05:40:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 710/1500: avg_reward = 178.50, avg_steps=685.90 (over 10 episodes)\n",
      "[05:58:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 720/1500: avg_reward = 324.00, avg_steps=969.40 (over 10 episodes)\n",
      "[06:10:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 730/1500: avg_reward = 149.50, avg_steps=680.60 (over 10 episodes)\n",
      "[06:24:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 740/1500: avg_reward = 173.00, avg_steps=756.20 (over 10 episodes)\n",
      "[06:41:32] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 750/1500: avg_reward = 243.00, avg_steps=892.00 (over 10 episodes)\n",
      "[06:55:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 760/1500: avg_reward = 185.50, avg_steps=795.10 (over 10 episodes)\n",
      "[07:11:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 770/1500: avg_reward = 272.50, avg_steps=916.80 (over 10 episodes)\n",
      "[07:26:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 780/1500: avg_reward = 188.50, avg_steps=849.30 (over 10 episodes)\n",
      "[07:39:25] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 790/1500: avg_reward = 211.50, avg_steps=796.90 (over 10 episodes)\n",
      "[07:54:57] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 800/1500: avg_reward = 244.00, avg_steps=922.70 (over 10 episodes)\n",
      "[08:06:57] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 810/1500: avg_reward = 164.50, avg_steps=713.80 (over 10 episodes)\n",
      "[08:21:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 820/1500: avg_reward = 166.50, avg_steps=840.70 (over 10 episodes)\n",
      "[08:35:23] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 830/1500: avg_reward = 224.50, avg_steps=860.30 (over 10 episodes)\n",
      "[08:49:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 840/1500: avg_reward = 213.00, avg_steps=830.50 (over 10 episodes)\n",
      "[09:02:45] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 850/1500: avg_reward = 209.00, avg_steps=844.50 (over 10 episodes)\n",
      "[09:14:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 860/1500: avg_reward = 198.00, avg_steps=709.20 (over 10 episodes)\n",
      "[09:27:20] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 870/1500: avg_reward = 226.50, avg_steps=812.20 (over 10 episodes)\n",
      "[09:38:21] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 880/1500: avg_reward = 175.00, avg_steps=689.70 (over 10 episodes)\n",
      "[09:49:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 890/1500: avg_reward = 199.50, avg_steps=713.60 (over 10 episodes)\n",
      "[10:01:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 900/1500: avg_reward = 177.00, avg_steps=756.90 (over 10 episodes)\n",
      "[10:12:51] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 910/1500: avg_reward = 186.00, avg_steps=724.60 (over 10 episodes)\n",
      "[10:25:41] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 920/1500: avg_reward = 228.00, avg_steps=796.30 (over 10 episodes)\n",
      "[10:37:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 930/1500: avg_reward = 137.00, avg_steps=693.00 (over 10 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-22d488b0495d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mENV\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug_verbosity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_of_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\examples\\shortcut.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, num_of_episodes)\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mqfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       \u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m       num_of_episodes=num_of_episodes)\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mDemo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_episodes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_video_to\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'demo.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mRun\u001b[1;34m(self, env, qfunc, policy, num_of_episodes)\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[1;34m\"\"\"Called at the end of each episode.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m   def OnCompletionCallback(\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\runner_impl.py\u001b[0m in \u001b[0;36m_protected_ProcessTransition\u001b[1;34m(self, qfunc, transition, step_idx)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_every_n_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m       brain.UpdateFromTransitions(\n\u001b[1;32m---> 99\u001b[1;33m         self._experience.Sample(self._experience_sample_batch_size))\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSampleFromHistory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mq_base\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mUpdateValues\u001b[1;34m(self, transitions)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[0mset\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m   def _SetActionValues(\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36m_SetActionValues\u001b[1;34m(self, states, actions, action_values, values)\u001b[0m\n\u001b[0;32m    277\u001b[0m       \u001b[0mdiscount_factor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mstrictly\u001b[0m \u001b[0mless\u001b[0m \u001b[0mthan\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mAffects\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mUpdateValues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m       \u001b[0mlearning_rate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0mcompletely\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mprevious\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mduring\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mUpdateValues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \"\"\"\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\q_base.py\u001b[0m in \u001b[0;36mGetValues\u001b[1;34m(self, states)\u001b[0m\n\u001b[0;32m    193\u001b[0m     \u001b[0mNotes\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0mtransitions\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mdifferent\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthere\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mconflict\u001b[0m \u001b[0msince\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mtransition\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mread\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mQFunction\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Workspace\\git\\deep_learning\\engine\\qfunc_impl.py\u001b[0m in \u001b[0;36m_protected_GetValues\u001b[1;34m(self, states)\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices_for_conversion_to_dense\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Workspace\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.ENV.debug_verbosity = 6\n",
    "pipeline.Train(num_of_episodes=1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN_TargetNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:59:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[20:59:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[20:59:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='SpaceInvaders-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    "    use_ddqn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Workspace\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "[21:04:35] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 10/3000: avg_reward = 200.50, avg_steps=797.50 (over 10 episodes)\n",
      "[21:09:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 20/3000: avg_reward = 142.50, avg_steps=696.90 (over 10 episodes)\n",
      "[21:14:14] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 30/3000: avg_reward = 101.00, avg_steps=613.00 (over 10 episodes)\n",
      "[21:19:31] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 40/3000: avg_reward = 117.50, avg_steps=621.80 (over 10 episodes)\n",
      "[21:27:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 50/3000: avg_reward = 195.00, avg_steps=805.20 (over 10 episodes)\n",
      "[21:34:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 60/3000: avg_reward = 175.50, avg_steps=708.90 (over 10 episodes)\n",
      "[21:45:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 70/3000: avg_reward = 196.00, avg_steps=802.30 (over 10 episodes)\n",
      "[21:56:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 80/3000: avg_reward = 147.00, avg_steps=748.50 (over 10 episodes)\n",
      "[22:12:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 90/3000: avg_reward = 189.50, avg_steps=849.50 (over 10 episodes)\n",
      "[22:31:11] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 100/3000: avg_reward = 200.00, avg_steps=854.30 (over 10 episodes)\n",
      "[22:50:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 110/3000: avg_reward = 228.00, avg_steps=768.40 (over 10 episodes)\n",
      "[23:08:24] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 120/3000: avg_reward = 126.00, avg_steps=634.30 (over 10 episodes)\n",
      "[23:26:06] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 130/3000: avg_reward = 145.00, avg_steps=590.70 (over 10 episodes)\n",
      "[23:56:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 140/3000: avg_reward = 179.50, avg_steps=788.40 (over 10 episodes)\n",
      "[00:32:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 150/3000: avg_reward = 168.50, avg_steps=783.70 (over 10 episodes)\n",
      "[01:03:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 160/3000: avg_reward = 181.00, avg_steps=692.20 (over 10 episodes)\n",
      "[01:37:37] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 170/3000: avg_reward = 215.50, avg_steps=778.40 (over 10 episodes)\n",
      "[02:05:34] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 180/3000: avg_reward = 160.50, avg_steps=673.00 (over 10 episodes)\n",
      "[02:36:50] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 190/3000: avg_reward = 281.00, avg_steps=780.60 (over 10 episodes)\n",
      "[03:12:00] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 200/3000: avg_reward = 289.50, avg_steps=938.50 (over 10 episodes)\n",
      "[03:37:20] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 210/3000: avg_reward = 162.00, avg_steps=715.00 (over 10 episodes)\n",
      "[04:00:17] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 220/3000: avg_reward = 186.50, avg_steps=683.20 (over 10 episodes)\n",
      "[04:24:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 230/3000: avg_reward = 168.00, avg_steps=775.20 (over 10 episodes)\n",
      "[04:45:01] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 240/3000: avg_reward = 177.50, avg_steps=686.10 (over 10 episodes)\n",
      "[05:05:22] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 250/3000: avg_reward = 193.50, avg_steps=743.40 (over 10 episodes)\n",
      "[05:24:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 260/3000: avg_reward = 185.50, avg_steps=765.10 (over 10 episodes)\n",
      "[05:42:37] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 270/3000: avg_reward = 178.00, avg_steps=754.40 (over 10 episodes)\n",
      "[06:00:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 280/3000: avg_reward = 179.00, avg_steps=743.30 (over 10 episodes)\n",
      "[06:18:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 290/3000: avg_reward = 179.50, avg_steps=715.00 (over 10 episodes)\n",
      "[06:35:54] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 300/3000: avg_reward = 167.00, avg_steps=712.30 (over 10 episodes)\n",
      "[06:51:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 310/3000: avg_reward = 125.00, avg_steps=611.40 (over 10 episodes)\n",
      "[07:09:04] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 320/3000: avg_reward = 155.00, avg_steps=727.20 (over 10 episodes)\n",
      "[07:28:41] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 330/3000: avg_reward = 219.00, avg_steps=806.50 (over 10 episodes)\n",
      "[07:43:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 340/3000: avg_reward = 167.00, avg_steps=605.60 (over 10 episodes)\n",
      "[08:00:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 350/3000: avg_reward = 147.00, avg_steps=677.00 (over 10 episodes)\n",
      "[08:14:40] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 360/3000: avg_reward = 134.50, avg_steps=600.60 (over 10 episodes)\n",
      "[08:38:27] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 370/3000: avg_reward = 259.50, avg_steps=969.00 (over 10 episodes)\n",
      "[08:55:02] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 380/3000: avg_reward = 155.00, avg_steps=693.30 (over 10 episodes)\n",
      "[09:13:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 390/3000: avg_reward = 273.00, avg_steps=790.50 (over 10 episodes)\n",
      "[09:34:42] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 400/3000: avg_reward = 240.00, avg_steps=893.60 (over 10 episodes)\n",
      "[09:53:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 410/3000: avg_reward = 195.00, avg_steps=808.40 (over 10 episodes)\n",
      "[10:11:10] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 420/3000: avg_reward = 207.00, avg_steps=760.60 (over 10 episodes)\n",
      "[10:29:26] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 430/3000: avg_reward = 202.50, avg_steps=808.40 (over 10 episodes)\n",
      "[10:47:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 440/3000: avg_reward = 210.00, avg_steps=783.80 (over 10 episodes)\n",
      "[11:07:45] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 450/3000: avg_reward = 254.50, avg_steps=931.40 (over 10 episodes)\n",
      "[11:23:33] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 460/3000: avg_reward = 177.50, avg_steps=722.80 (over 10 episodes)\n",
      "[11:38:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 470/3000: avg_reward = 157.50, avg_steps=693.20 (over 10 episodes)\n",
      "[11:56:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 480/3000: avg_reward = 217.00, avg_steps=820.70 (over 10 episodes)\n",
      "[12:16:47] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 490/3000: avg_reward = 223.50, avg_steps=929.10 (over 10 episodes)\n",
      "[12:30:44] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 500/3000: avg_reward = 137.00, avg_steps=647.50 (over 10 episodes)\n",
      "[12:44:56] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 510/3000: avg_reward = 160.00, avg_steps=657.50 (over 10 episodes)\n",
      "[12:59:39] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 520/3000: avg_reward = 237.50, avg_steps=694.00 (over 10 episodes)\n",
      "[13:16:51] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 530/3000: avg_reward = 232.00, avg_steps=814.90 (over 10 episodes)\n",
      "[13:32:32] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 540/3000: avg_reward = 163.00, avg_steps=745.30 (over 10 episodes)\n",
      "[13:50:03] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 550/3000: avg_reward = 264.00, avg_steps=831.30 (over 10 episodes)\n",
      "[14:02:23] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 560/3000: avg_reward = 137.00, avg_steps=592.00 (over 10 episodes)\n",
      "[14:16:50] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 570/3000: avg_reward = 159.00, avg_steps=697.50 (over 10 episodes)\n",
      "[14:30:18] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 580/3000: avg_reward = 168.00, avg_steps=662.90 (over 10 episodes)\n",
      "[14:45:53] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 590/3000: avg_reward = 246.50, avg_steps=778.00 (over 10 episodes)\n",
      "[14:59:44] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 600/3000: avg_reward = 150.50, avg_steps=700.20 (over 10 episodes)\n",
      "[15:11:43] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 610/3000: avg_reward = 136.50, avg_steps=604.70 (over 10 episodes)\n",
      "[15:28:08] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 620/3000: avg_reward = 235.00, avg_steps=838.40 (over 10 episodes)\n",
      "[15:43:00] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 630/3000: avg_reward = 229.00, avg_steps=755.70 (over 10 episodes)\n",
      "[15:58:13] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 640/3000: avg_reward = 230.50, avg_steps=778.50 (over 10 episodes)\n",
      "[16:12:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 650/3000: avg_reward = 148.00, avg_steps=766.60 (over 10 episodes)\n",
      "[16:28:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 660/3000: avg_reward = 186.00, avg_steps=824.30 (over 10 episodes)\n",
      "[16:40:45] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 670/3000: avg_reward = 139.00, avg_steps=642.90 (over 10 episodes)\n",
      "[16:54:19] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 680/3000: avg_reward = 175.00, avg_steps=712.80 (over 10 episodes)\n",
      "[17:06:52] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 690/3000: avg_reward = 140.00, avg_steps=664.60 (over 10 episodes)\n",
      "[17:21:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 700/3000: avg_reward = 214.50, avg_steps=768.30 (over 10 episodes)\n",
      "[17:33:01] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 710/3000: avg_reward = 161.00, avg_steps=622.80 (over 10 episodes)\n",
      "[17:44:28] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 720/3000: avg_reward = 143.50, avg_steps=633.50 (over 10 episodes)\n",
      "[17:56:46] c:\\Workspace\\git\\qpylib\\logging.py:44 Episode 730/3000: avg_reward = 127.50, avg_steps=681.20 (over 10 episodes)\n"
     ]
    }
   ],
   "source": [
    "pipeline.Train(num_of_episodes=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using qfunc implementation: DDQN\n",
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[20:33:30] c:\\Workspace\\git\\qpylib\\logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='SpaceInvaders-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    "    use_ddqn=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.Train(num_of_episodes=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
