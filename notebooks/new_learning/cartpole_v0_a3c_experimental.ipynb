{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path c:\\Workspace\\git\\ added to PYTHON_PATH.\n",
      "Path c:\\Workspace\\git\\deep_learning added to PYTHON_PATH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "reloaded: a3c_impl\n",
      "forwarded symbol: a3c_impl\n",
      "reloaded: base\n",
      "forwarded symbol: base\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: interval_world_env\n",
      "forwarded symbol: interval_world_env\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: other_runners\n",
      "forwarded symbol: other_runners\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_extension_impl\n",
      "forwarded symbol: runner_extension_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n",
      "reloaded: running_environment\n",
      "forwarded symbol: running_environment\n",
      "reloaded: shortcut\n",
      "forwarded symbol: shortcut\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.02318315,  0.58242713,  0.08012372, -0.42810967]]),\n",
       "  array([[ 0.03937273,  0.00450853, -0.04028056, -0.01137044]]),\n",
       "  array([[-1.22393435, -0.94591479, -0.10243092, -0.30642735]]),\n",
       "  array([[-1.00146948, -0.89419425, -0.05840249, -0.10293925]]),\n",
       "  array([[1.26898112, 0.90985765, 0.05516422, 0.0211102 ]]),\n",
       "  array([[ 0.07734736,  0.38238581,  0.00808368, -0.43614473]]),\n",
       "  array([[ 0.32898555,  0.54686486,  0.01403588, -0.30861282]]),\n",
       "  array([[ 0.55113367,  0.40094633,  0.01053951, -0.12118495]]),\n",
       "  array([[ 0.2304714 ,  0.34016697,  0.01694664, -0.02476815]]),\n",
       "  array([[ 0.81349645,  1.31489248,  0.14548658, -0.00726391]]),\n",
       "  array([[-0.50330022,  0.36431455, -0.05797559, -0.07904508]]),\n",
       "  array([[ 1.29932225,  1.26296664,  0.08618098, -0.25595331]]),\n",
       "  array([[ 0.07301362,  0.34381018, -0.01331721, -0.5010894 ]])],\n",
       " [array([[0.77688442, 0.18271239, 0.01830044, 0.45438987]]),\n",
       "  array([[ 0.07219074, -0.21979337, -0.0127479 ,  0.3597516 ]]),\n",
       "  array([[-0.01872194, -0.01780948,  0.03422948,  0.07417699]]),\n",
       "  array([[-0.03013217,  0.19765872,  0.05044142, -0.19798146]]),\n",
       "  array([[ 0.04643257, -0.19702157, -0.01337837,  0.30328672]]),\n",
       "  array([[0.71234048, 0.37587919, 0.06810788, 0.40175974]]),\n",
       "  array([[ 0.00592257, -0.21748067, -0.00195983,  0.32134995]])]]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_states_1 = states[0][:5]\n",
    "use_states_2 = states[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_states_1 = [\n",
    "    np.array([[ 0.02318315,  0.58242713,  0.08012372, -0.42810967]]),\n",
    "    np.array([[ 0.03937273,  0.00450853, -0.04028056, -0.01137044]]),\n",
    "    np.array([[-1.22393435, -0.94591479, -0.10243092, -0.30642735]]),\n",
    "    np.array([[-1.00146948, -0.89419425, -0.05840249, -0.10293925]]),\n",
    "    np.array([[1.26898112, 0.90985765, 0.05516422, 0.0211102 ]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_states_2 = [\n",
    "    np.array([[0.77688442, 0.18271239, 0.01830044, 0.45438987]]),\n",
    "    np.array([[ 0.07219074, -0.21979337, -0.0127479 ,  0.3597516 ]]),\n",
    "    np.array([[-0.01872194, -0.01780948,  0.03422948,  0.07417699]]),\n",
    "    np.array([[-0.03013217,  0.19765872,  0.05044142, -0.19798146]]),\n",
    "    np.array([[ 0.04643257, -0.19702157, -0.01337837,  0.30328672]])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_environment.ForceCpuForTheRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:40:24] c:\\Workspace\\git\\deep_learning\\engine\\a3c_impl.py:46 WARNING: only one A3C instance can be active; the previous instance <deep_learning.engine.a3c_impl.A3C object at 0x00000171DA675F60> is now deactivated.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64  # used in qfunc and runner.\n",
    "envs = [\n",
    "    environment_impl.GymEnvironment(gym.make('CartPole-v0')),\n",
    "    environment_impl.GymEnvironment(gym.make('CartPole-v0')),\n",
    "    environment_impl.GymEnvironment(gym.make('CartPole-v0')),\n",
    "    environment_impl.GymEnvironment(gym.make('CartPole-v0')),\n",
    "    environment_impl.GymEnvironment(gym.make('CartPole-v0')),\n",
    "]\n",
    "brain = a3c_impl.A3C(\n",
    "    model=a3c_impl.CreateModel(\n",
    "        state_shape=env.GetStateShape(),\n",
    "        action_space_size=env.GetActionSpaceSize(),\n",
    "        hidden_layer_sizes=(12,),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logging.ENV.debug_verbosity = 6\n",
    "runner = other_runners.MultiEnvironmentRunner()\n",
    "runner.AddCallback(runner_extension_impl.ProgressTracer(report_every_num_of_episodes=20))\n",
    "runner.AddCallback(runner_extension_impl.ValueTracer(trace_states=use_states_1 + use_states_2, trace_actions=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:11] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 20/20000: avg_reward = 49.45, avg_steps=49.45 (over 20 episodes)\n",
      "[20:41:12] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 40/20000: avg_reward = 50.55, avg_steps=50.55 (over 20 episodes)\n",
      "[20:41:13] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 60/20000: avg_reward = 49.60, avg_steps=49.60 (over 20 episodes)\n",
      "[20:41:14] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 80/20000: avg_reward = 48.60, avg_steps=48.60 (over 20 episodes)\n",
      "[20:41:15] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 100/20000: avg_reward = 57.85, avg_steps=57.85 (over 20 episodes)\n",
      "[20:41:16] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 120/20000: avg_reward = 56.40, avg_steps=56.40 (over 20 episodes)\n",
      "[20:41:17] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 140/20000: avg_reward = 50.95, avg_steps=50.95 (over 20 episodes)\n",
      "[20:41:18] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 160/20000: avg_reward = 50.85, avg_steps=50.85 (over 20 episodes)\n",
      "[20:41:19] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 180/20000: avg_reward = 58.80, avg_steps=58.80 (over 20 episodes)\n",
      "[20:41:20] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 200/20000: avg_reward = 49.35, avg_steps=49.35 (over 20 episodes)\n",
      "[20:41:21] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 220/20000: avg_reward = 84.80, avg_steps=84.80 (over 20 episodes)\n",
      "[20:41:22] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 240/20000: avg_reward = 51.30, avg_steps=51.30 (over 20 episodes)\n",
      "[20:41:23] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 260/20000: avg_reward = 53.45, avg_steps=53.45 (over 20 episodes)\n",
      "[20:41:25] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 280/20000: avg_reward = 80.25, avg_steps=80.25 (over 20 episodes)\n",
      "[20:41:26] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 300/20000: avg_reward = 64.80, avg_steps=64.80 (over 20 episodes)\n",
      "[20:41:27] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 320/20000: avg_reward = 49.90, avg_steps=49.90 (over 20 episodes)\n",
      "[20:41:28] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 340/20000: avg_reward = 49.25, avg_steps=49.25 (over 20 episodes)\n",
      "[20:41:29] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 360/20000: avg_reward = 48.85, avg_steps=48.85 (over 20 episodes)\n",
      "[20:41:30] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 380/20000: avg_reward = 49.10, avg_steps=49.10 (over 20 episodes)\n",
      "[20:41:31] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 400/20000: avg_reward = 48.25, avg_steps=48.25 (over 20 episodes)\n",
      "[20:41:32] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 420/20000: avg_reward = 48.80, avg_steps=48.80 (over 20 episodes)\n",
      "[20:41:33] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 440/20000: avg_reward = 48.65, avg_steps=48.65 (over 20 episodes)\n",
      "[20:41:33] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 460/20000: avg_reward = 49.25, avg_steps=49.25 (over 20 episodes)\n",
      "[20:41:34] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 480/20000: avg_reward = 48.35, avg_steps=48.35 (over 20 episodes)\n",
      "[20:41:35] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 500/20000: avg_reward = 47.45, avg_steps=47.45 (over 20 episodes)\n",
      "[20:41:36] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 520/20000: avg_reward = 49.25, avg_steps=49.25 (over 20 episodes)\n",
      "[20:41:37] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 540/20000: avg_reward = 55.15, avg_steps=55.15 (over 20 episodes)\n",
      "[20:41:38] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 560/20000: avg_reward = 50.90, avg_steps=50.90 (over 20 episodes)\n",
      "[20:41:39] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 580/20000: avg_reward = 48.45, avg_steps=48.45 (over 20 episodes)\n",
      "[20:41:40] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 600/20000: avg_reward = 49.80, avg_steps=49.80 (over 20 episodes)\n",
      "[20:41:41] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 620/20000: avg_reward = 47.45, avg_steps=47.45 (over 20 episodes)\n",
      "[20:41:42] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 640/20000: avg_reward = 48.25, avg_steps=48.25 (over 20 episodes)\n",
      "[20:41:43] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 660/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:41:44] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 680/20000: avg_reward = 49.75, avg_steps=49.75 (over 20 episodes)\n",
      "[20:41:45] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 700/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:41:46] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 720/20000: avg_reward = 49.10, avg_steps=49.10 (over 20 episodes)\n",
      "[20:41:47] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 740/20000: avg_reward = 49.10, avg_steps=49.10 (over 20 episodes)\n",
      "[20:41:48] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 760/20000: avg_reward = 48.85, avg_steps=48.85 (over 20 episodes)\n",
      "[20:41:49] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 780/20000: avg_reward = 48.30, avg_steps=48.30 (over 20 episodes)\n",
      "[20:41:50] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 800/20000: avg_reward = 47.20, avg_steps=47.20 (over 20 episodes)\n",
      "[20:41:50] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 820/20000: avg_reward = 46.90, avg_steps=46.90 (over 20 episodes)\n",
      "[20:41:51] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 840/20000: avg_reward = 48.85, avg_steps=48.85 (over 20 episodes)\n",
      "[20:41:52] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 860/20000: avg_reward = 46.05, avg_steps=46.05 (over 20 episodes)\n",
      "[20:41:53] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 880/20000: avg_reward = 47.70, avg_steps=47.70 (over 20 episodes)\n",
      "[20:41:54] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 900/20000: avg_reward = 47.70, avg_steps=47.70 (over 20 episodes)\n",
      "[20:41:55] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 920/20000: avg_reward = 49.30, avg_steps=49.30 (over 20 episodes)\n",
      "[20:41:56] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 940/20000: avg_reward = 48.80, avg_steps=48.80 (over 20 episodes)\n",
      "[20:41:57] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 960/20000: avg_reward = 48.25, avg_steps=48.25 (over 20 episodes)\n",
      "[20:41:58] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 980/20000: avg_reward = 49.95, avg_steps=49.95 (over 20 episodes)\n",
      "[20:41:59] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1000/20000: avg_reward = 49.15, avg_steps=49.15 (over 20 episodes)\n",
      "[20:42:00] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1020/20000: avg_reward = 49.25, avg_steps=49.25 (over 20 episodes)\n",
      "[20:42:01] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1040/20000: avg_reward = 48.65, avg_steps=48.65 (over 20 episodes)\n",
      "[20:42:02] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1060/20000: avg_reward = 48.85, avg_steps=48.85 (over 20 episodes)\n",
      "[20:42:03] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1080/20000: avg_reward = 51.95, avg_steps=51.95 (over 20 episodes)\n",
      "[20:42:04] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1100/20000: avg_reward = 47.95, avg_steps=47.95 (over 20 episodes)\n",
      "[20:42:05] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1120/20000: avg_reward = 49.35, avg_steps=49.35 (over 20 episodes)\n",
      "[20:42:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1140/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:42:07] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1160/20000: avg_reward = 49.35, avg_steps=49.35 (over 20 episodes)\n",
      "[20:42:08] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1180/20000: avg_reward = 49.40, avg_steps=49.40 (over 20 episodes)\n",
      "[20:42:09] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1200/20000: avg_reward = 49.85, avg_steps=49.85 (over 20 episodes)\n",
      "[20:42:10] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1220/20000: avg_reward = 50.10, avg_steps=50.10 (over 20 episodes)\n",
      "[20:42:11] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1240/20000: avg_reward = 49.75, avg_steps=49.75 (over 20 episodes)\n",
      "[20:42:13] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1260/20000: avg_reward = 50.40, avg_steps=50.40 (over 20 episodes)\n",
      "[20:42:14] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1280/20000: avg_reward = 49.80, avg_steps=49.80 (over 20 episodes)\n",
      "[20:42:15] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1300/20000: avg_reward = 50.25, avg_steps=50.25 (over 20 episodes)\n",
      "[20:42:16] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1320/20000: avg_reward = 61.10, avg_steps=61.10 (over 20 episodes)\n",
      "[20:42:17] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1340/20000: avg_reward = 51.25, avg_steps=51.25 (over 20 episodes)\n",
      "[20:42:18] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1360/20000: avg_reward = 50.10, avg_steps=50.10 (over 20 episodes)\n",
      "[20:42:19] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1380/20000: avg_reward = 59.25, avg_steps=59.25 (over 20 episodes)\n",
      "[20:42:20] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1400/20000: avg_reward = 66.10, avg_steps=66.10 (over 20 episodes)\n",
      "[20:42:21] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1420/20000: avg_reward = 57.25, avg_steps=57.25 (over 20 episodes)\n",
      "[20:42:23] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1440/20000: avg_reward = 60.40, avg_steps=60.40 (over 20 episodes)\n",
      "[20:42:24] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1460/20000: avg_reward = 72.65, avg_steps=72.65 (over 20 episodes)\n",
      "[20:42:25] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1480/20000: avg_reward = 64.00, avg_steps=64.00 (over 20 episodes)\n",
      "[20:42:26] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1500/20000: avg_reward = 49.45, avg_steps=49.45 (over 20 episodes)\n",
      "[20:42:27] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1520/20000: avg_reward = 49.25, avg_steps=49.25 (over 20 episodes)\n",
      "[20:42:28] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1540/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:42:29] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1560/20000: avg_reward = 48.80, avg_steps=48.80 (over 20 episodes)\n",
      "[20:42:30] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1580/20000: avg_reward = 49.70, avg_steps=49.70 (over 20 episodes)\n",
      "[20:42:31] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1600/20000: avg_reward = 49.70, avg_steps=49.70 (over 20 episodes)\n",
      "[20:42:32] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1620/20000: avg_reward = 51.40, avg_steps=51.40 (over 20 episodes)\n",
      "[20:42:34] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1640/20000: avg_reward = 68.35, avg_steps=68.35 (over 20 episodes)\n",
      "[20:42:35] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1660/20000: avg_reward = 62.50, avg_steps=62.50 (over 20 episodes)\n",
      "[20:42:36] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1680/20000: avg_reward = 49.15, avg_steps=49.15 (over 20 episodes)\n",
      "[20:42:37] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1700/20000: avg_reward = 49.40, avg_steps=49.40 (over 20 episodes)\n",
      "[20:42:38] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1720/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:42:39] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1740/20000: avg_reward = 50.45, avg_steps=50.45 (over 20 episodes)\n",
      "[20:42:40] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1760/20000: avg_reward = 49.45, avg_steps=49.45 (over 20 episodes)\n",
      "[20:42:41] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1780/20000: avg_reward = 49.00, avg_steps=49.00 (over 20 episodes)\n",
      "[20:42:42] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1800/20000: avg_reward = 49.10, avg_steps=49.10 (over 20 episodes)\n",
      "[20:42:43] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1820/20000: avg_reward = 49.95, avg_steps=49.95 (over 20 episodes)\n",
      "[20:42:45] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1840/20000: avg_reward = 49.85, avg_steps=49.85 (over 20 episodes)\n",
      "[20:42:46] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1860/20000: avg_reward = 49.65, avg_steps=49.65 (over 20 episodes)\n",
      "[20:42:47] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1880/20000: avg_reward = 49.10, avg_steps=49.10 (over 20 episodes)\n",
      "[20:42:48] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1900/20000: avg_reward = 48.80, avg_steps=48.80 (over 20 episodes)\n",
      "[20:42:49] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1920/20000: avg_reward = 52.05, avg_steps=52.05 (over 20 episodes)\n",
      "[20:42:51] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1940/20000: avg_reward = 54.60, avg_steps=54.60 (over 20 episodes)\n",
      "[20:42:52] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1960/20000: avg_reward = 52.05, avg_steps=52.05 (over 20 episodes)\n",
      "[20:42:53] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 1980/20000: avg_reward = 50.00, avg_steps=50.00 (over 20 episodes)\n",
      "[20:42:54] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2000/20000: avg_reward = 63.30, avg_steps=63.30 (over 20 episodes)\n",
      "[20:42:56] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2020/20000: avg_reward = 78.60, avg_steps=78.60 (over 20 episodes)\n",
      "[20:42:57] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2040/20000: avg_reward = 52.45, avg_steps=52.45 (over 20 episodes)\n",
      "[20:42:58] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2060/20000: avg_reward = 49.90, avg_steps=49.90 (over 20 episodes)\n",
      "[20:42:59] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2080/20000: avg_reward = 49.95, avg_steps=49.95 (over 20 episodes)\n",
      "[20:43:00] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2100/20000: avg_reward = 48.45, avg_steps=48.45 (over 20 episodes)\n",
      "[20:43:01] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2120/20000: avg_reward = 49.65, avg_steps=49.65 (over 20 episodes)\n",
      "[20:43:02] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2140/20000: avg_reward = 49.20, avg_steps=49.20 (over 20 episodes)\n",
      "[20:43:03] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2160/20000: avg_reward = 48.55, avg_steps=48.55 (over 20 episodes)\n",
      "[20:43:04] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2180/20000: avg_reward = 49.60, avg_steps=49.60 (over 20 episodes)\n",
      "[20:43:05] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2200/20000: avg_reward = 49.65, avg_steps=49.65 (over 20 episodes)\n",
      "[20:43:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2220/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:43:07] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2240/20000: avg_reward = 50.25, avg_steps=50.25 (over 20 episodes)\n",
      "[20:43:09] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2260/20000: avg_reward = 50.70, avg_steps=50.70 (over 20 episodes)\n",
      "[20:43:10] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2280/20000: avg_reward = 49.15, avg_steps=49.15 (over 20 episodes)\n",
      "[20:43:11] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2300/20000: avg_reward = 50.35, avg_steps=50.35 (over 20 episodes)\n",
      "[20:43:12] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2320/20000: avg_reward = 51.55, avg_steps=51.55 (over 20 episodes)\n",
      "[20:43:13] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2340/20000: avg_reward = 52.40, avg_steps=52.40 (over 20 episodes)\n",
      "[20:43:14] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2360/20000: avg_reward = 49.85, avg_steps=49.85 (over 20 episodes)\n",
      "[20:43:15] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2380/20000: avg_reward = 51.60, avg_steps=51.60 (over 20 episodes)\n",
      "[20:43:16] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2400/20000: avg_reward = 49.70, avg_steps=49.70 (over 20 episodes)\n",
      "[20:43:17] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2420/20000: avg_reward = 51.55, avg_steps=51.55 (over 20 episodes)\n",
      "[20:43:18] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2440/20000: avg_reward = 49.80, avg_steps=49.80 (over 20 episodes)\n",
      "[20:43:19] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2460/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:43:20] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2480/20000: avg_reward = 49.15, avg_steps=49.15 (over 20 episodes)\n",
      "[20:43:22] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2500/20000: avg_reward = 49.65, avg_steps=49.65 (over 20 episodes)\n",
      "[20:43:23] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2520/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:43:24] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2540/20000: avg_reward = 49.60, avg_steps=49.60 (over 20 episodes)\n",
      "[20:43:25] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2560/20000: avg_reward = 51.15, avg_steps=51.15 (over 20 episodes)\n",
      "[20:43:26] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2580/20000: avg_reward = 49.80, avg_steps=49.80 (over 20 episodes)\n",
      "[20:43:27] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2600/20000: avg_reward = 50.80, avg_steps=50.80 (over 20 episodes)\n",
      "[20:43:28] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2620/20000: avg_reward = 49.60, avg_steps=49.60 (over 20 episodes)\n",
      "[20:43:29] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2640/20000: avg_reward = 48.80, avg_steps=48.80 (over 20 episodes)\n",
      "[20:43:30] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2660/20000: avg_reward = 52.15, avg_steps=52.15 (over 20 episodes)\n",
      "[20:43:32] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2680/20000: avg_reward = 55.30, avg_steps=55.30 (over 20 episodes)\n",
      "[20:43:33] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2700/20000: avg_reward = 57.25, avg_steps=57.25 (over 20 episodes)\n",
      "[20:43:34] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2720/20000: avg_reward = 51.70, avg_steps=51.70 (over 20 episodes)\n",
      "[20:43:35] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2740/20000: avg_reward = 78.65, avg_steps=78.65 (over 20 episodes)\n",
      "[20:43:36] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2760/20000: avg_reward = 49.70, avg_steps=49.70 (over 20 episodes)\n",
      "[20:43:37] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2780/20000: avg_reward = 50.45, avg_steps=50.45 (over 20 episodes)\n",
      "[20:43:39] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2800/20000: avg_reward = 50.10, avg_steps=50.10 (over 20 episodes)\n",
      "[20:43:40] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2820/20000: avg_reward = 49.50, avg_steps=49.50 (over 20 episodes)\n",
      "[20:43:41] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2840/20000: avg_reward = 48.70, avg_steps=48.70 (over 20 episodes)\n",
      "[20:43:42] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2860/20000: avg_reward = 49.00, avg_steps=49.00 (over 20 episodes)\n",
      "[20:43:43] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2880/20000: avg_reward = 50.80, avg_steps=50.80 (over 20 episodes)\n",
      "[20:43:44] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2900/20000: avg_reward = 49.80, avg_steps=49.80 (over 20 episodes)\n",
      "[20:43:45] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2920/20000: avg_reward = 49.65, avg_steps=49.65 (over 20 episodes)\n",
      "[20:43:46] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2940/20000: avg_reward = 50.05, avg_steps=50.05 (over 20 episodes)\n",
      "[20:43:47] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2960/20000: avg_reward = 49.10, avg_steps=49.10 (over 20 episodes)\n",
      "[20:43:48] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 2980/20000: avg_reward = 49.00, avg_steps=49.00 (over 20 episodes)\n",
      "[20:43:49] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3000/20000: avg_reward = 49.40, avg_steps=49.40 (over 20 episodes)\n",
      "[20:43:50] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3020/20000: avg_reward = 49.70, avg_steps=49.70 (over 20 episodes)\n",
      "[20:43:51] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3040/20000: avg_reward = 50.55, avg_steps=50.55 (over 20 episodes)\n",
      "[20:43:52] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3060/20000: avg_reward = 52.05, avg_steps=52.05 (over 20 episodes)\n",
      "[20:43:53] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3080/20000: avg_reward = 50.20, avg_steps=50.20 (over 20 episodes)\n",
      "[20:43:54] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3100/20000: avg_reward = 51.25, avg_steps=51.25 (over 20 episodes)\n",
      "[20:43:55] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3120/20000: avg_reward = 49.55, avg_steps=49.55 (over 20 episodes)\n",
      "[20:43:56] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3140/20000: avg_reward = 49.05, avg_steps=49.05 (over 20 episodes)\n",
      "[20:43:57] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3160/20000: avg_reward = 50.25, avg_steps=50.25 (over 20 episodes)\n",
      "[20:43:58] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3180/20000: avg_reward = 50.05, avg_steps=50.05 (over 20 episodes)\n",
      "[20:43:59] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3200/20000: avg_reward = 51.35, avg_steps=51.35 (over 20 episodes)\n",
      "[20:44:00] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3220/20000: avg_reward = 49.60, avg_steps=49.60 (over 20 episodes)\n",
      "[20:44:01] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3240/20000: avg_reward = 51.90, avg_steps=51.90 (over 20 episodes)\n",
      "[20:44:02] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3260/20000: avg_reward = 54.00, avg_steps=54.00 (over 20 episodes)\n",
      "[20:44:03] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3280/20000: avg_reward = 54.90, avg_steps=54.90 (over 20 episodes)\n",
      "[20:44:05] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3300/20000: avg_reward = 54.80, avg_steps=54.80 (over 20 episodes)\n",
      "[20:44:06] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3320/20000: avg_reward = 55.00, avg_steps=55.00 (over 20 episodes)\n",
      "[20:44:07] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3340/20000: avg_reward = 51.80, avg_steps=51.80 (over 20 episodes)\n",
      "[20:44:08] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3360/20000: avg_reward = 64.00, avg_steps=64.00 (over 20 episodes)\n",
      "[20:44:09] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3380/20000: avg_reward = 59.75, avg_steps=59.75 (over 20 episodes)\n",
      "[20:44:10] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3400/20000: avg_reward = 60.65, avg_steps=60.65 (over 20 episodes)\n",
      "[20:44:11] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3420/20000: avg_reward = 48.85, avg_steps=48.85 (over 20 episodes)\n",
      "[20:44:13] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3440/20000: avg_reward = 48.20, avg_steps=48.20 (over 20 episodes)\n",
      "[20:44:14] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3460/20000: avg_reward = 50.10, avg_steps=50.10 (over 20 episodes)\n",
      "[20:44:15] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3480/20000: avg_reward = 49.80, avg_steps=49.80 (over 20 episodes)\n",
      "[20:44:16] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3500/20000: avg_reward = 50.25, avg_steps=50.25 (over 20 episodes)\n",
      "[20:44:17] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3520/20000: avg_reward = 49.65, avg_steps=49.65 (over 20 episodes)\n",
      "[20:44:19] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3540/20000: avg_reward = 50.35, avg_steps=50.35 (over 20 episodes)\n",
      "[20:44:20] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3560/20000: avg_reward = 52.45, avg_steps=52.45 (over 20 episodes)\n",
      "[20:44:21] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3580/20000: avg_reward = 49.95, avg_steps=49.95 (over 20 episodes)\n",
      "[20:44:22] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3600/20000: avg_reward = 51.90, avg_steps=51.90 (over 20 episodes)\n",
      "[20:44:24] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3620/20000: avg_reward = 49.75, avg_steps=49.75 (over 20 episodes)\n",
      "[20:44:25] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3640/20000: avg_reward = 51.05, avg_steps=51.05 (over 20 episodes)\n",
      "[20:44:26] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3660/20000: avg_reward = 51.25, avg_steps=51.25 (over 20 episodes)\n",
      "[20:44:27] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3680/20000: avg_reward = 50.75, avg_steps=50.75 (over 20 episodes)\n",
      "[20:44:28] c:\\Workspace\\git\\deep_learning\\engine\\runner_extension_impl.py:58 Episode 3700/20000: avg_reward = 49.90, avg_steps=49.90 (over 20 episodes)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "runner.Run(envs=envs, brain=brain, policy=policy_impl.GreedyPolicyWithRandomness(epsilon=0.1), num_of_episodes=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
