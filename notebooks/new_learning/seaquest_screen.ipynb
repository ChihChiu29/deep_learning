{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path /workspace/external/ added to PYTHON_PATH.\n",
      "Path /workspace/external/deep_learning added to PYTHON_PATH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_extension_impl\n",
      "forwarded symbol: runner_extension_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n",
      "reloaded: shortcut\n",
      "forwarded symbol: shortcut\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 05:01:21.543677 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0730 05:01:21.549850 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0730 05:01:21.554471 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0730 05:01:21.581989 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0730 05:01:21.587639 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0730 05:01:21.764420 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:01:22] logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[05:01:22] logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[05:01:22] logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='Seaquest-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:06:27] logging.py:44 Episode 10/1000: avg_reward = 46.00, avg_steps=669.50 (over 10 episodes)\n",
      "[05:11:09] logging.py:44 Episode 20/1000: avg_reward = 36.00, avg_steps=630.40 (over 10 episodes)\n",
      "[05:15:49] logging.py:44 Episode 30/1000: avg_reward = 44.00, avg_steps=623.10 (over 10 episodes)\n",
      "[05:20:05] logging.py:44 Episode 40/1000: avg_reward = 34.00, avg_steps=553.30 (over 10 episodes)\n",
      "[05:25:05] logging.py:44 Episode 50/1000: avg_reward = 50.00, avg_steps=647.80 (over 10 episodes)\n",
      "[05:31:21] logging.py:44 Episode 60/1000: avg_reward = 120.00, avg_steps=835.40 (over 10 episodes)\n",
      "[05:40:01] logging.py:44 Episode 70/1000: avg_reward = 200.00, avg_steps=1095.20 (over 10 episodes)\n",
      "[05:48:21] logging.py:44 Episode 80/1000: avg_reward = 190.00, avg_steps=1059.10 (over 10 episodes)\n",
      "[05:55:59] logging.py:44 Episode 90/1000: avg_reward = 188.00, avg_steps=1004.50 (over 10 episodes)\n",
      "[06:05:06] logging.py:44 Episode 100/1000: avg_reward = 250.00, avg_steps=1195.30 (over 10 episodes)\n",
      "[06:14:30] logging.py:44 Episode 110/1000: avg_reward = 278.00, avg_steps=1243.40 (over 10 episodes)\n",
      "[06:26:33] logging.py:44 Episode 120/1000: avg_reward = 334.00, avg_steps=1423.90 (over 10 episodes)\n",
      "[06:39:08] logging.py:44 Episode 130/1000: avg_reward = 392.00, avg_steps=1587.90 (over 10 episodes)\n",
      "[06:52:29] logging.py:44 Episode 140/1000: avg_reward = 428.00, avg_steps=1719.90 (over 10 episodes)\n",
      "[07:05:33] logging.py:44 Episode 150/1000: avg_reward = 386.00, avg_steps=1644.30 (over 10 episodes)\n",
      "[07:19:27] logging.py:44 Episode 160/1000: avg_reward = 430.00, avg_steps=1763.20 (over 10 episodes)\n",
      "[07:33:18] logging.py:44 Episode 170/1000: avg_reward = 448.00, avg_steps=1809.70 (over 10 episodes)\n",
      "[07:48:27] logging.py:44 Episode 180/1000: avg_reward = 492.00, avg_steps=1974.80 (over 10 episodes)\n",
      "[08:05:08] logging.py:44 Episode 190/1000: avg_reward = 526.00, avg_steps=2141.90 (over 10 episodes)\n",
      "[08:21:39] logging.py:44 Episode 200/1000: avg_reward = 524.00, avg_steps=2134.70 (over 10 episodes)\n",
      "[08:38:52] logging.py:44 Episode 210/1000: avg_reward = 554.00, avg_steps=2222.10 (over 10 episodes)\n",
      "[08:55:35] logging.py:44 Episode 220/1000: avg_reward = 544.00, avg_steps=2187.00 (over 10 episodes)\n",
      "[09:12:45] logging.py:44 Episode 230/1000: avg_reward = 574.00, avg_steps=2260.80 (over 10 episodes)\n",
      "[09:30:14] logging.py:44 Episode 240/1000: avg_reward = 560.00, avg_steps=2300.90 (over 10 episodes)\n",
      "[09:46:39] logging.py:44 Episode 250/1000: avg_reward = 540.00, avg_steps=2152.90 (over 10 episodes)\n",
      "[10:01:56] logging.py:44 Episode 260/1000: avg_reward = 556.00, avg_steps=2174.70 (over 10 episodes)\n",
      "[10:15:31] logging.py:44 Episode 270/1000: avg_reward = 534.00, avg_steps=2081.20 (over 10 episodes)\n",
      "[10:31:51] logging.py:44 Episode 280/1000: avg_reward = 668.00, avg_steps=2526.50 (over 10 episodes)\n",
      "[10:45:57] logging.py:44 Episode 290/1000: avg_reward = 566.00, avg_steps=2215.20 (over 10 episodes)\n",
      "[11:01:48] logging.py:44 Episode 300/1000: avg_reward = 646.00, avg_steps=2436.90 (over 10 episodes)\n",
      "[11:16:48] logging.py:44 Episode 310/1000: avg_reward = 544.00, avg_steps=2151.20 (over 10 episodes)\n",
      "[11:33:05] logging.py:44 Episode 320/1000: avg_reward = 650.00, avg_steps=2396.90 (over 10 episodes)\n",
      "[11:48:03] logging.py:44 Episode 330/1000: avg_reward = 590.00, avg_steps=2201.10 (over 10 episodes)\n",
      "[12:04:51] logging.py:44 Episode 340/1000: avg_reward = 648.00, avg_steps=2461.40 (over 10 episodes)\n",
      "[12:20:33] logging.py:44 Episode 350/1000: avg_reward = 610.00, avg_steps=2325.50 (over 10 episodes)\n",
      "[12:38:54] logging.py:44 Episode 360/1000: avg_reward = 714.00, avg_steps=2644.00 (over 10 episodes)\n",
      "[12:56:49] logging.py:44 Episode 370/1000: avg_reward = 704.00, avg_steps=2619.10 (over 10 episodes)\n",
      "[13:13:48] logging.py:44 Episode 380/1000: avg_reward = 668.00, avg_steps=2484.00 (over 10 episodes)\n",
      "[13:30:16] logging.py:44 Episode 390/1000: avg_reward = 672.00, avg_steps=2454.50 (over 10 episodes)\n",
      "[13:47:06] logging.py:44 Episode 400/1000: avg_reward = 670.00, avg_steps=2455.50 (over 10 episodes)\n",
      "[14:05:08] logging.py:44 Episode 410/1000: avg_reward = 718.00, avg_steps=2591.60 (over 10 episodes)\n",
      "[14:19:17] logging.py:44 Episode 420/1000: avg_reward = 560.00, avg_steps=2052.80 (over 10 episodes)\n",
      "[14:36:12] logging.py:44 Episode 430/1000: avg_reward = 664.00, avg_steps=2374.70 (over 10 episodes)\n",
      "[14:53:02] logging.py:44 Episode 440/1000: avg_reward = 688.00, avg_steps=2479.80 (over 10 episodes)\n",
      "[15:08:29] logging.py:44 Episode 450/1000: avg_reward = 612.00, avg_steps=2265.50 (over 10 episodes)\n",
      "[15:24:15] logging.py:44 Episode 460/1000: avg_reward = 640.00, avg_steps=2316.30 (over 10 episodes)\n",
      "[15:40:42] logging.py:44 Episode 470/1000: avg_reward = 678.00, avg_steps=2384.80 (over 10 episodes)\n",
      "[15:57:08] logging.py:44 Episode 480/1000: avg_reward = 700.00, avg_steps=2432.20 (over 10 episodes)\n",
      "[16:13:08] logging.py:44 Episode 490/1000: avg_reward = 666.00, avg_steps=2377.20 (over 10 episodes)\n",
      "[16:31:35] logging.py:44 Episode 500/1000: avg_reward = 740.00, avg_steps=2592.00 (over 10 episodes)\n",
      "[16:52:00] logging.py:44 Episode 510/1000: avg_reward = 696.00, avg_steps=2520.80 (over 10 episodes)\n",
      "[17:14:56] logging.py:44 Episode 520/1000: avg_reward = 716.00, avg_steps=2548.00 (over 10 episodes)\n",
      "[17:36:18] logging.py:44 Episode 530/1000: avg_reward = 688.00, avg_steps=2439.20 (over 10 episodes)\n",
      "[17:56:02] logging.py:44 Episode 540/1000: avg_reward = 618.00, avg_steps=2240.30 (over 10 episodes)\n",
      "[18:14:24] logging.py:44 Episode 550/1000: avg_reward = 578.00, avg_steps=2064.40 (over 10 episodes)\n",
      "[18:33:56] logging.py:44 Episode 560/1000: avg_reward = 656.00, avg_steps=2350.50 (over 10 episodes)\n",
      "[18:52:09] logging.py:44 Episode 570/1000: avg_reward = 610.00, avg_steps=2160.70 (over 10 episodes)\n",
      "[19:10:50] logging.py:44 Episode 580/1000: avg_reward = 584.00, avg_steps=2137.10 (over 10 episodes)\n",
      "[19:29:05] logging.py:44 Episode 590/1000: avg_reward = 630.00, avg_steps=2271.10 (over 10 episodes)\n",
      "[19:44:47] logging.py:44 Episode 600/1000: avg_reward = 550.00, avg_steps=1999.20 (over 10 episodes)\n",
      "[20:06:28] logging.py:44 Episode 610/1000: avg_reward = 794.00, avg_steps=2723.90 (over 10 episodes)\n",
      "[20:26:47] logging.py:44 Episode 620/1000: avg_reward = 696.00, avg_steps=2441.40 (over 10 episodes)\n",
      "[20:43:45] logging.py:44 Episode 630/1000: avg_reward = 610.00, avg_steps=2180.20 (over 10 episodes)\n",
      "[21:02:40] logging.py:44 Episode 640/1000: avg_reward = 696.00, avg_steps=2422.70 (over 10 episodes)\n",
      "[21:20:05] logging.py:44 Episode 650/1000: avg_reward = 642.00, avg_steps=2221.20 (over 10 episodes)\n",
      "[21:36:50] logging.py:44 Episode 660/1000: avg_reward = 704.00, avg_steps=2447.90 (over 10 episodes)\n",
      "[21:52:52] logging.py:44 Episode 670/1000: avg_reward = 678.00, avg_steps=2425.90 (over 10 episodes)\n",
      "[22:07:15] logging.py:44 Episode 680/1000: avg_reward = 630.00, avg_steps=2191.10 (over 10 episodes)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-384546f2ef5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/workspace/external/deep_learning/examples/shortcut.py\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(self, num_of_episodes)\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mqfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m       num_of_episodes=num_of_episodes)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mDemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_of_episodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_video_to\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'demo.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36mRun\u001b[0;34m(self, env, qfunc, policy, num_of_episodes)\u001b[0m\n\u001b[1;32m    429\u001b[0m           \u001b[0mqfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m           \u001b[0mtransition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtran\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m           step_idx=step_idx)\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtran\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/runner_impl.py\u001b[0m in \u001b[0;36m_protected_ProcessTransition\u001b[0;34m(self, qfunc, transition, step_idx)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_every_n_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m       qfunc.UpdateValues(\n\u001b[0;32m---> 99\u001b[0;31m         self._experience.Sample(self._experience_sample_batch_size))\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mSampleFromHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36mUpdateValues\u001b[0;34m(self, transitions)\u001b[0m\n\u001b[1;32m    323\u001b[0m         values=values)\n\u001b[1;32m    324\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetActionValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_new_action_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36m_SetActionValues\u001b[0;34m(self, states, actions, action_values, values)\u001b[0m\n\u001b[1;32m    267\u001b[0m       \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SetValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/q_base.py\u001b[0m in \u001b[0;36m_SetValues\u001b[0;34m(self, states, values)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \"\"\"\n\u001b[1;32m    221\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SET: (%s) <- %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protected_SetValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/qfunc_impl.py\u001b[0m in \u001b[0;36m_protected_SetValues\u001b[0;34m(self, states, values)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mq_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQValues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   ) -> None:\n\u001b[0;32m--> 238\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protected_SetValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_target_network_every_num_of_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/external/deep_learning/engine/qfunc_impl.py\u001b[0m in \u001b[0;36m_protected_SetValues\u001b[0;34m(self, states, values)\u001b[0m\n\u001b[1;32m    168\u001b[0m   ) -> None:\n\u001b[1;32m    169\u001b[0m     self._model.fit(\n\u001b[0;32m--> 170\u001b[0;31m       states, values, batch_size=self._training_batch_size, verbose=0)\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline.Train(num_of_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
