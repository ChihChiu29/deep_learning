{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path /workspace/external/ added to PYTHON_PATH.\n",
      "Path /workspace/external/deep_learning added to PYTHON_PATH.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notebook_init.py imported and reloaded\n",
      "reloaded: circular_world_env\n",
      "forwarded symbol: circular_world_env\n",
      "reloaded: environment_impl\n",
      "forwarded symbol: environment_impl\n",
      "reloaded: guided_environments\n",
      "forwarded symbol: guided_environments\n",
      "reloaded: gym\n",
      "forwarded symbol: gym\n",
      "reloaded: keras\n",
      "forwarded symbol: keras\n",
      "reloaded: layers\n",
      "forwarded symbol: layers\n",
      "reloaded: logging\n",
      "forwarded symbol: logging\n",
      "reloaded: model_builder\n",
      "forwarded symbol: model_builder\n",
      "reloaded: models\n",
      "forwarded symbol: models\n",
      "reloaded: numpy\n",
      "forwarded symbol: numpy\n",
      "reloaded: numpy_util\n",
      "forwarded symbol: numpy_util\n",
      "reloaded: optimizers\n",
      "forwarded symbol: optimizers\n",
      "reloaded: policy_impl\n",
      "forwarded symbol: policy_impl\n",
      "reloaded: q_base\n",
      "forwarded symbol: q_base\n",
      "reloaded: qfunc_impl\n",
      "forwarded symbol: qfunc_impl\n",
      "reloaded: runner_extension_impl\n",
      "forwarded symbol: runner_extension_impl\n",
      "reloaded: runner_impl\n",
      "forwarded symbol: runner_impl\n",
      "reloaded: shortcut\n",
      "forwarded symbol: shortcut\n"
     ]
    }
   ],
   "source": [
    "ReloadProject('deep_learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 05:01:21.543677 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0730 05:01:21.549850 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0730 05:01:21.554471 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0730 05:01:21.581989 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0730 05:01:21.587639 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0730 05:01:21.764420 140658362005312 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:01:22] logging.py:44 Using qfunc implementation: DQN_TargetNetwork\n",
      "[05:01:22] logging.py:44 Using policy implementation: GreedyPolicyWithDecreasingRandomness\n",
      "[05:01:22] logging.py:44 Using runner implementation: ExperienceReplayRunner\n"
     ]
    }
   ],
   "source": [
    "pipeline = shortcut.ScreenLearningPipeline(\n",
    "    gym_env_name='Seaquest-v0',\n",
    "    report_every_num_of_episodes=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:06:27] logging.py:44 Episode 10/1000: avg_reward = 46.00, avg_steps=669.50 (over 10 episodes)\n",
      "[05:11:09] logging.py:44 Episode 20/1000: avg_reward = 36.00, avg_steps=630.40 (over 10 episodes)\n",
      "[05:15:49] logging.py:44 Episode 30/1000: avg_reward = 44.00, avg_steps=623.10 (over 10 episodes)\n",
      "[05:20:05] logging.py:44 Episode 40/1000: avg_reward = 34.00, avg_steps=553.30 (over 10 episodes)\n",
      "[05:25:05] logging.py:44 Episode 50/1000: avg_reward = 50.00, avg_steps=647.80 (over 10 episodes)\n",
      "[05:31:21] logging.py:44 Episode 60/1000: avg_reward = 120.00, avg_steps=835.40 (over 10 episodes)\n",
      "[05:40:01] logging.py:44 Episode 70/1000: avg_reward = 200.00, avg_steps=1095.20 (over 10 episodes)\n",
      "[05:48:21] logging.py:44 Episode 80/1000: avg_reward = 190.00, avg_steps=1059.10 (over 10 episodes)\n",
      "[05:55:59] logging.py:44 Episode 90/1000: avg_reward = 188.00, avg_steps=1004.50 (over 10 episodes)\n",
      "[06:05:06] logging.py:44 Episode 100/1000: avg_reward = 250.00, avg_steps=1195.30 (over 10 episodes)\n",
      "[06:14:30] logging.py:44 Episode 110/1000: avg_reward = 278.00, avg_steps=1243.40 (over 10 episodes)\n",
      "[06:26:33] logging.py:44 Episode 120/1000: avg_reward = 334.00, avg_steps=1423.90 (over 10 episodes)\n",
      "[06:39:08] logging.py:44 Episode 130/1000: avg_reward = 392.00, avg_steps=1587.90 (over 10 episodes)\n",
      "[06:52:29] logging.py:44 Episode 140/1000: avg_reward = 428.00, avg_steps=1719.90 (over 10 episodes)\n",
      "[07:05:33] logging.py:44 Episode 150/1000: avg_reward = 386.00, avg_steps=1644.30 (over 10 episodes)\n",
      "[07:19:27] logging.py:44 Episode 160/1000: avg_reward = 430.00, avg_steps=1763.20 (over 10 episodes)\n",
      "[07:33:18] logging.py:44 Episode 170/1000: avg_reward = 448.00, avg_steps=1809.70 (over 10 episodes)\n",
      "[07:48:27] logging.py:44 Episode 180/1000: avg_reward = 492.00, avg_steps=1974.80 (over 10 episodes)\n",
      "[08:05:08] logging.py:44 Episode 190/1000: avg_reward = 526.00, avg_steps=2141.90 (over 10 episodes)\n",
      "[08:21:39] logging.py:44 Episode 200/1000: avg_reward = 524.00, avg_steps=2134.70 (over 10 episodes)\n",
      "[08:38:52] logging.py:44 Episode 210/1000: avg_reward = 554.00, avg_steps=2222.10 (over 10 episodes)\n",
      "[08:55:35] logging.py:44 Episode 220/1000: avg_reward = 544.00, avg_steps=2187.00 (over 10 episodes)\n",
      "[09:12:45] logging.py:44 Episode 230/1000: avg_reward = 574.00, avg_steps=2260.80 (over 10 episodes)\n",
      "[09:30:14] logging.py:44 Episode 240/1000: avg_reward = 560.00, avg_steps=2300.90 (over 10 episodes)\n",
      "[09:46:39] logging.py:44 Episode 250/1000: avg_reward = 540.00, avg_steps=2152.90 (over 10 episodes)\n",
      "[10:01:56] logging.py:44 Episode 260/1000: avg_reward = 556.00, avg_steps=2174.70 (over 10 episodes)\n",
      "[10:15:31] logging.py:44 Episode 270/1000: avg_reward = 534.00, avg_steps=2081.20 (over 10 episodes)\n",
      "[10:31:51] logging.py:44 Episode 280/1000: avg_reward = 668.00, avg_steps=2526.50 (over 10 episodes)\n",
      "[10:45:57] logging.py:44 Episode 290/1000: avg_reward = 566.00, avg_steps=2215.20 (over 10 episodes)\n",
      "[11:01:48] logging.py:44 Episode 300/1000: avg_reward = 646.00, avg_steps=2436.90 (over 10 episodes)\n",
      "[11:16:48] logging.py:44 Episode 310/1000: avg_reward = 544.00, avg_steps=2151.20 (over 10 episodes)\n",
      "[11:33:05] logging.py:44 Episode 320/1000: avg_reward = 650.00, avg_steps=2396.90 (over 10 episodes)\n",
      "[11:48:03] logging.py:44 Episode 330/1000: avg_reward = 590.00, avg_steps=2201.10 (over 10 episodes)\n",
      "[12:04:51] logging.py:44 Episode 340/1000: avg_reward = 648.00, avg_steps=2461.40 (over 10 episodes)\n",
      "[12:20:33] logging.py:44 Episode 350/1000: avg_reward = 610.00, avg_steps=2325.50 (over 10 episodes)\n",
      "[12:38:54] logging.py:44 Episode 360/1000: avg_reward = 714.00, avg_steps=2644.00 (over 10 episodes)\n",
      "[12:56:49] logging.py:44 Episode 370/1000: avg_reward = 704.00, avg_steps=2619.10 (over 10 episodes)\n",
      "[13:13:48] logging.py:44 Episode 380/1000: avg_reward = 668.00, avg_steps=2484.00 (over 10 episodes)\n",
      "[13:30:16] logging.py:44 Episode 390/1000: avg_reward = 672.00, avg_steps=2454.50 (over 10 episodes)\n",
      "[13:47:06] logging.py:44 Episode 400/1000: avg_reward = 670.00, avg_steps=2455.50 (over 10 episodes)\n",
      "[14:05:08] logging.py:44 Episode 410/1000: avg_reward = 718.00, avg_steps=2591.60 (over 10 episodes)\n",
      "[14:19:17] logging.py:44 Episode 420/1000: avg_reward = 560.00, avg_steps=2052.80 (over 10 episodes)\n",
      "[14:36:12] logging.py:44 Episode 430/1000: avg_reward = 664.00, avg_steps=2374.70 (over 10 episodes)\n",
      "[14:53:02] logging.py:44 Episode 440/1000: avg_reward = 688.00, avg_steps=2479.80 (over 10 episodes)\n",
      "[15:08:29] logging.py:44 Episode 450/1000: avg_reward = 612.00, avg_steps=2265.50 (over 10 episodes)\n",
      "[15:24:15] logging.py:44 Episode 460/1000: avg_reward = 640.00, avg_steps=2316.30 (over 10 episodes)\n",
      "[15:40:42] logging.py:44 Episode 470/1000: avg_reward = 678.00, avg_steps=2384.80 (over 10 episodes)\n",
      "[15:57:08] logging.py:44 Episode 480/1000: avg_reward = 700.00, avg_steps=2432.20 (over 10 episodes)\n",
      "[16:13:08] logging.py:44 Episode 490/1000: avg_reward = 666.00, avg_steps=2377.20 (over 10 episodes)\n",
      "[16:31:35] logging.py:44 Episode 500/1000: avg_reward = 740.00, avg_steps=2592.00 (over 10 episodes)\n",
      "[16:52:00] logging.py:44 Episode 510/1000: avg_reward = 696.00, avg_steps=2520.80 (over 10 episodes)\n",
      "[17:14:56] logging.py:44 Episode 520/1000: avg_reward = 716.00, avg_steps=2548.00 (over 10 episodes)\n",
      "[17:36:18] logging.py:44 Episode 530/1000: avg_reward = 688.00, avg_steps=2439.20 (over 10 episodes)\n",
      "[17:56:02] logging.py:44 Episode 540/1000: avg_reward = 618.00, avg_steps=2240.30 (over 10 episodes)\n"
     ]
    }
   ],
   "source": [
    "pipeline.Train(num_of_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
